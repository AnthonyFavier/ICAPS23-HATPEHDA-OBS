\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{adjustbox}
\usepackage{multirow}

\usepackage{enumitem}
\newlist{myitemize}{itemize}{1}
\setlist[myitemize,1]{label=\textbullet,leftmargin=0pt}

% \newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing} 

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%
%\nocopyright

% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title (Will They Know? Integrating Theory of Mind \\ in Human Aware Task Planning for Collaborative Robots)
% /Author (Anthony Favier, Shashank Shekhar, Rachid Alami)
/Author Authors)
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title
{
% old-1. Human-Aware Planning with Communication: \\ While Keeping Itself in Human's Shoes a Robot Plans for Both 
% Perspective Taking and Its Use for Communication in Planning: 
% Old Title 2:Planning with Communication Empowered by Situation Assessment:
% \\ 
% A Robot Builds Collaborative Plans
% while Keeping Itself in Human's Shoes
% Considering Theory of Mind in  Human-Aware Task Planning \\ for a Collaborative Robot
% Robust Task Planning for Human-Aware Collaborative Robots via Theory of Mind
Will They Know? Integrating Theory of Mind \\ in Human Aware Task Planning for Collaborative Robots
% Anticipating Human's Situation Assessment during \\ Robot Task Planning to Improve the Collaboration
% Robust Planning for Human-Robot Joint Tasks \\ with Explicit Reasoning on Human Mental State
%While Keeping Itself in Human's Shoes a Robot Produce Collaborative Plans 
%is in Human's Shoes   
% old-2. Towards a robot partner reducing\\ ambiguities using perspective taking
}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Anthony Favier\textsuperscript{\rm 1,2},
    % Shashank Shekhar\textsuperscript{\rm 1},
    % Rachid Alami\textsuperscript{\rm 1,2}
    Authors
}
\affiliations{
    % Afiliations
    % \textsuperscript{\rm 1}LAAS-CNRS, Universite de Toulouse, Toulouse, France\\
    % \textsuperscript{\rm 2}{Artificial and Natural Intelligence Toulouse Institute (ANITI)}

    % % email address must be in roman text type, not monospace or sans serif
    % \{anthony.favier, sshekhar, rachid.alami\}@laas.fr
    % Affiliations
}

\begin{document}

\include{commands}

\maketitle

\begin{abstract}
% OLD
% Building on earlier work about human-aware planning and execution, we describe a new approach that:
% (a) models the robot’s capabilities to compute the spatial perspective of its human partner while following their activities; 
% and using that, (b) it estimates the evolution of the mental state of its human partner to perform robust planning for the human’s and robot’s joint tasks with a known objective to achieve.
% More specifically, our approach formalizes standard execution-time observability conventions based abstractly on situation assessment. Using this formalization during planning, it captures the evolution of human’s and robot’s beliefs to
% deal with belief divergences that could arise in practice and
% affect plan execution. While anticipating such a divergence, the approach decides \textit{if} and \textit{when} belief alignment is needed
% and achieves it using communication.  
% The proposed approach is sound and complete and generates robust plans for more realistic and challenging problems, and it uses communication effectively,
% i.e., without the robot giving unnecessary information that a collaborative human can observe or infer in practice.
% NEW

% anthony-v1
% When planning for a collaborative task, sharing the plan before execution is a common practice and assumption that may be verbally costly and even restrict the human's behavior. A recent approach named HATP/EHDA lifts this assumption and plans the robot actions while estimating the future human's behavior, without constraining it.
% For a proper interaction, Theory of Mind (ToM) must be considered by the robot. Several works integrate ToM at execution time to adapt online the robot's behavior to the human. However, such intelligent reactions may not always be enough. In some scenarios, ToM should be considered at planning time to be proactive. 
% Hence, we describe a new conceptual approach that: (a) plans the robot actions to achieve a joint shared goal with a human considered as uncontrollable,
% (b) manages the human's belief distinctly to predict their behavior and adapt the robot's one accordingly,
% (c) anticipates the upcoming spatial perspective of the human to better estimate their belief. 
% More specifically, we explain where and how Situation Assessment is inserted in the planning process. And we show, thanks to this addition, how ambiguous situations due to estimated false human beliefs can be detected and avoided by different means (communication or delay robot actions).  
% This conceptual approach has been implemented, and some qualitative and experimental results are discussed to show \textit{its potential}.

% anthony-v2
% Theory of Mind (ToM) must be considered by the robot to interact properly with humans. Several works integrate ToM at execution time to adapt online the robot's behavior to unplanned human initiatives. However, such online approaches may not always be enough and in some scenarios ToM should be considered during offline planning for the robot to be proactive. 
% On the other hand, when planning for a collaborative task, sharing the plan before execution is a common practice and assumption that has benefits but may be verbally costly and even restrict the human's behavior. Hence, adapting online the robot's behavior may involve replanning and sharing again the plan which can worsen the interaction quality.
% Recent offline planners provide more latitude to humans by discarding the idea of shared plan and considering the human as \textit{uncontrollable}. Indeed, they plan only for the robot's behavior while estimating future humans' behavior.
% Building on existing work, we describe a new offline planning approach that formalizes ToM to maintain agents' distinct beliefs 
% (a) plans the robot actions to achieve a joint shared goal with a human considered as uncontrollable,
% (b) manages the human's belief distinctly to predict their behavior and adapt the robot's one accordingly,
% (c) anticipates the upcoming spatial perspective of the human to better estimate their belief. 
% More specifically, we explain where and how Situation Assessment is inserted in the planning process. And we show, thanks to this addition, how ambiguous situations due to estimated false human beliefs can be detected and avoided by different means (communication or delay robot actions).  
% This conceptual approach has been implemented, and some qualitative and experimental results are discussed to show \textit{its potential}.

% anthony-v3
% It has been established that it is beneficial to endow a robot with the ability to plan for both itself and the human it collaborates with. Such capability can be considered as using Theory of Mind (ToM).
% % Collaborative robots must consider Theory of Mind (ToM) to interact properly with humans. 
% Several works integrate ToM at execution time to adapt online the robot's behavior to unplanned human initiatives. 
% However, already considering ToM during offline planning may offer smarter and proactive alternatives to the robot.
% % Recent offline planners provide more latitude to humans by considering them as \textit{uncontrollable} and discarding the idea of shared plan. Indeed, they plan only for the robot's behavior which is implicitly coordinated with the estimated non-deterministic human's behavior.
% A recent scheme (HATP/EHDA) explicitly models humans as non-controllable agents and aims to only plan for the robot's behavior which is implicitly coordinated with the estimated non-deterministic human's behavior.
% Building on this existing work, we propose a contribution which will allow a better offline estimation and exploitation of the human's mental state. 
% Some Situation Assessment (SA) processes are modeled and inserted in the planning process, distinctly for each agent.  
% These models are based on the \textit{co-presence} and two types of action effects, (1) facts that are inferred while performing an action or observing an action being executed, and (2) facts that can be observed by an agent at anytime. Thanks to this addition, we show how ambiguous situations due to estimated false human beliefs can be detected and avoided by different means (communication or delay/postpone robot actions).
% % This conceptual approach has been implemented, proved to be sound and complete, and some qualitative and experimental results are discussed on three novel scenarios to show \textit{its potential}.
% We implement our new conceptual approach, prove its soundness and completeness, discuss its effectiveness qualitatively, and show experimental results on three novel domains.

% anthony-v4
It is essential for a collaborative robot to consider Theory of Mind (ToM) when interacting with humans. Several works integrate ToM when executing joint plans to adapt online the robot's behavior to unplanned human initiatives. 
However, having such considerations while planning \textit{offline} for the robot by taking humans' behaviors into account; can offer it smarter and more proactive alternatives.
Recent offline planning schemes like HATP/EHDA explicitly model humans as uncontrollable agents and aim to plan for robot's actions while coordinating them \textit{implicitly} with the estimated human's behaviors. These schemes are suitable for and would benefit from integrating ToM concepts in a principled way to anticipate human behavior better.
In this paper, based on an existing scheme, we propose a new contribution --- an offline planning approach allowing for better estimation and exploitation of the human's mental state. 
More precisely, our approach formalizes for each agent simple yet elegant situation assessment models, which are managed at the symbolic level and inserted in an existing planning framework. 
They are based on the notion of {\em co-presence}
% Some Situation Assessment processes are modeled and inserted in the planning process, distinctly for each agent. These models are based on \textit{co-presence} 
and two types of \textit{action effects}: (1) facts that are inferred while performing an action or observing an action being executed, and (2) facts that can be observed by an agent at any time. Thanks to this addition, we show how ambiguous situations due to estimated false human beliefs are detected and can be avoided by different means, e.g., communication and delayed robot actions.
We implemented our new conceptual approach, prove its soundness and completeness, discuss its effectiveness qualitatively, and show experimental results on three novel domains.


% \textbf{*****}

% shashank-v1
% Considering Theory of Mind (ToM) when executing a human-robot shared plan enables a robot to adapt its behavior online. Recent work addresses an online replanning approach while tracking the progress of the execution of the overall task from both its own and humans' perspectives. 
% If it finds a new plan, it communicates with the humans their part and other essential task details, and continue to execute its part. 
% Here, ToM is used to handle uncertainties caused mainly due to humans' \textit{unplanned} initiatives at run time, for example, their temporary absence or distractions, which prohibit humans to entail knowledge about the exact execution status. 
% Such online approaches are heavily dependent on plan sharing that can be costly, and committing every time to a new plan can be restrictive to humans or sometimes it may not be possible. Recent offline solvers lift the requirement of shared planning such that they only plan for the robot's behavior considering humans as an \textit{uncontrollable} agent and hence only anticipate their non-deterministic behaviors. Such estimation leads to an \textit{implicitly} coordinated joint plan given a (shared) goal. 
% In this paper, building on an existing approach, we describe a new planning system that uses ToM to maintain human's-robot's individual mental models while anticipating humans' future initiatives (more precisely, their spatial perspective). 
% % for managing their mental models. 
% In particular, we formalize a simple version of the situation assessment (SA) with perspective-taking and use it in our new planning approach; while, more precisely, proposing where and how SA is inserted in the planning process. 
% As a result, the solver produces more \textit{robust} robot plans while estimating the human mental state, encouraging the robot to communicate about relevant humans' false beliefs, but only minimally and in a principled way. 
% We implement our new planning approach, prove its soundness and completeness, discuss its effectiveness qualitatively, and show experimental results in three novel scenarios. 

% shashank-v2
% It has been established that endowing a robot with the ability to reason and plan in the presence of collaborative humans is beneficial as it allows the robot to elaborate more pertinent shared plans. And Theory of Mind (ToM) is essential when executing such a plan to enable a robot to adapt its behavior online.
% It is used to handle uncertainties caused mainly due to humans' unplanned run-time initiatives.
% % , e.g., their temporary absence or distractions, which prohibit humans from entailing the exact execution status. 
% Recent work on online replanning tracks the progress of the task execution from both humans' and robots' perspectives. 
% Once replanning succeeds, the human part of the new plan is shared along with other essential task details to continue. However, frequent sharing can be costly and restrictive process for humans.     
% % Online approaches are dependent on plan sharing, which can be costly, and committing every time to a new plan can be restrictive. 
% To the contrary, recent offline solvers like HATP/EHDA provide more latitude to humans by discarding the idea of shared planning as they plan only for the robot's behavior. But, in addition, they consider humans \textit{uncontrollable} and hence anticipate their non-deterministic behaviors and generate \textit{implicitly} coordinated joint plans for a (non-)shared goal. 
% In this paper, we describe a new offline planning approach based on HATP/EHDA, which formalizes ToM to maintain agents' individual mental models, helping better anticipate humans' future initiatives.
% % (their spatial perspective via \textit{coexistence}). 
% % for managing their mental models. 
% In particular, we formalize a \textit{version} of situation assessment (SA) process that is based on co-presence and managed at symbolic level.
% % , and use SA in our new planning approach. 
% We then propose precisely where and how SA can be inserted and utilized in planning. As a result, our solver produces better coordinated plans, while estimating the human mental state encourages the robot to communicate to correct humans' false beliefs, but only minimally and in a principled way. 
% We implement our new planning approach, prove its soundness and completeness, discuss its effectiveness qualitatively, and show experimental results on three novel scenarios. 
\end{abstract}

\section{Introduction}
% With the increasing penetration of sensor networks, advancements in robotic technology, the Internet of Things (IoT), etc., multi-robot systems are becoming ubiquitous, and the complexity of the tasks these autonomous robots can handle individually or together as a team is constantly increasing.
% However, robots collaborating and (or) interacting with humans, for example, a robot hands-over, a robot achieving a joint task with the help of a human or receiving some routine help from a human, etc., is seldom seen in our day-to-day life, but will soon become ubiquitous, too.   
Human-Robot Collaboration (HRC) is a current research focus due to the growing number of robot-assisted applications~\cite{selvaggio2021autonomy}.
%removed - kragic2021effective
Collaborative robots add clear value to manufacturing by promising to boost productivity and improve working conditions~\cite{johannsmeier2016hierarchical}, while there are clear advantages of allowing robots and humans to work together in (future) workshops~\cite{hoffman2007effects,coupete2015gesture}.
% . In assembly lines, there is a strong economic advantage to allowing robots and humans to collaborate~\cite{nikolaidis2012human,coupete2015gesture}. 
Therefore, autonomy from the robot's side is quite beneficial with respect to the collaboration's efficiency and efficacy.  
However, effective integration of robots in workplaces is non-trivial and primarily raises two types of research challenges: (1) perceiving humans' behavior and predicting their intended goals/tasks \cite{cheng2020towards}, and (2) planning for the robot's behavior while also considering humans' operators, which is also broadly known as human-aware planning and decision-making~\cite{CirilloKS09,CramerKD21,UnhelkarLS20}, and negotiating for role allocation~\cite{roncone2017transparent}. 
In this paper, we focus on the latter. In particular, human-aware interaction planning for a subclass of HR collaborative tasks, namely sequential tasks with known joint task objectives~\cite{cheng2021human,UnhelkarLS19,buisan:hal-03684211}, which can be applied in real-world domains like a robot assisting human surgeons~\cite{jacob2013collaboration}, collaborating with humans at workshops~\cite{unhelkar2018human}, and helping astronauts~\cite{diftler2011robonaut}. 

For seamless human-robot collaboration, it is essential to integrate a task planning framework~\cite{lallement2018hatp} or a scheduling framework~\cite{ferreira2021scheduling} in an \text{online} joint plan execution scheme. 
In~\cite{PupaS21}, planning information is dynamically integrated into the scheduling framework, while Devin and Alami~(\citeyear{devin2016implemented}) (henceforth, DA) propose to consider ToM when interacting with humans to react to their 
% \textit{unplanned} 
\textit{unpredictable}
run-time initiatives,
% . 
% The DA's  framework adapts online the robot's behavior to \textit{unplanned} human initiatives, 
e.g., their temporary absence or inattention, which prohibit them from entailing the exact execution status. 
However, replanning frameworks generally come with a supervisor (in~\cite{johannsmeier2016hierarchical} or DA's), which reduces flexibility and makes humans more stressed as, roughly speaking, robots routinely inform or request current status like ground truths, refined plans, etc.
% The framework's supervisor keeps an eye on the progress of the execution from both humans' and robots' perspectives using ToM. If replanning succeeds, the human part of the new plan is shared along with other essential task details to continue. 

Recently proposed planning frameworks as in~\cite{buisan:hal-03684211,UnhelkarLS19} provide more flexibility to the human. These offline approaches accept as input one or more of HR task knowledge, human activities and their intentions, environment states, model specifications, shared goals, etc. 
Unlike the frameworks proposed in~\cite{alami2006toward,lallement2018hatp,roncone2017transparent}, they generate robot plans that are implicitly coordinated with estimated human behaviors while considering humans as {\em uncontrollable} agents. 
% However, those frameworks rather produce explicitly coordinated HR joint plans that are legible and acceptable by humans --- they are assumed to be controllable in some sense, while relying more on the replanning aspect. %%% MOVED TO BKGD
So one straightforward approach could be to have a mechanism that somehow adapts the very idea of a recently proposed flexible scheme in the DA's online framework. 

In this work, we propose to estimate humans' \textit{predictable} execution-time initiatives in offline planning. And for that, we formalize a simple yet elegant version of Theory of Mind and integrate it with the process of the existing HATP/EHDA framework~\cite{buisan:hal-03684211} (extends  HATP~\cite{lallement2018hatp,CirilloKS09}). 
One main reason to select HATP/EHDA as the basis for realizing our version of ToM is that, it uses factored task model and specifications that make the problem more compact and amenable to   reasoning, which is less likely with frameworks~\cite{unhelkar2019learning} employing flat models~\cite{levine2014concurrent,unhelkar2019learning}.
We claim that our contribution can offer a collaborative robot more clever and proactive alternatives to coordinate and interact with humans.

%%%%%% PREVIOUS BEING SHORTENED BELOW %%%%%%%%
% We formalize situation assessment (SA) models for participating agents. 
% The models are based on the notion of {\em co-presence} and two types of \textit{action effects}, which are: (1) facts that are inferred by an agent while performing an action, or observing an action being executed by another agent, and (2) facts that can be observed by an agent at any time.
% We note that agents' SA models are managed at the symbolic level and appended to the HATP/EHDA framework to be conceptualized~\cite{buisan:hal-03684211}. 
% More specifically, we upgrade the framework's existing planning scheme to utilize these models as a part of the deliberation process.   

% This integration allows for estimating ambiguous situations due to estimated \textit{false} human beliefs such that they are detected and avoided through different means. 
% % For example: (1) the robot communicates with the human to correct their false belief, and (2) the robot (on purpose) delays its action creating a divergence.
% For example: (1) the robot communicates with the human to correct their false belief, and (2) the robot delays its action on purpose so the human is aware of key facts through perception, without communicating.
% The latter is more to show proactive behavior, where the robot can estimate and reason about actions so that humans can be aware of the {\em key updates}, ensuring that humans could rather perceive the situation. 
% However, we thoroughly study only the \textit{first} case showing that our new planning scheme captures the evolution of individual agents' beliefs and estimates belief divergences that arise in practice. 
% It then decides if and when belief alignment is needed and achieves it by communicating with humans, but only minimally and in a principled way. 
% % To add a different flavor, 
% For the \textit{second} case, we provide just a pilot study and intend to investigate in the future.   

% We implement this novel concept for HR collaborative planning, prove its soundness and completeness, discuss its effectiveness qualitatively, and show experimental results on three novel domains. 

% The paper is structured as follows: 
% We discuss the related work in the next section, and then we introduce the section that briefly describes the underlying framework, which we extend essentially to conceptualize our ideas. Next, we describe models based on simple ToM, discussing how SA models are formalized based on the notion of observability. The next section follows this and discusses how simple ToM is used to update the agents' beliefs. 
% We further elaborate this section: it first gives a workflow using agents' SA models for the belief updates, and then discusses how ambiguities in the agents' understanding of the world are computed and to verify if they are relevant.
% It then discusses approaches to fix these ambiguities and some inline work for the future.
% Next to this section, we provide proof of soundness and completeness of the updated planning scheme ({omitted} due to the page constraints and {provided} in the appendix), followed by the evaluation section discussing both qualitative and quantitative results. 
% We conclude the paper with a discussion.  

%%%%%%%% END PREVIOUS %%%%%%%%%%%%%

We formalize situation assessment (SA) models for the participating agents. 
They are based on the notion of {\em co-presence} and two types of \textit{action effects}: (1) facts that are inferred by an agent while performing an action, or by observing another agent acting, and (2) facts that can be observed in a state at any time.
We extend the existing framework's encoding and upgrade its planning scheme to utilize these models at the symbolic level in the deliberation process. This allows for anticipating ambiguous situations due to estimated \textit{false} human beliefs. 
These situations are detected and avoided through different means such as: (1) minimal communication with the human to align their belief, or 
(2) delaying a robot's action on purpose ensuring that humans could perceive the {\em key updates} rather than being communicated.
The latter shows proactive robot behavior but it is currently limited so we just provide a pilot study and intend to investigate in the future. However, we thoroughly study only the former case showing how our planning scheme captures a pertinent evolution of individual agents' beliefs and decides if and when belief alignment is needed.


% The paper is structured as follows: 
% Related works are discussed in Section~\ref{sec:related-work}, and Section~\ref{sec:under-framework} briefly describes the underlying framework, which we extend to conceptualize our ideas. 
% Section~\ref{sec:model-tom} describes simple ToM models by formalizing SA based on the notion of observability. 
% Section~\ref{sec:beliefs-updates} follows this section, and discusses how agents' beliefs are updated. It first includes a description of how our SA models are used in a workflow to maintain the agent's beliefs. And then, it discusses how ambiguities in one's belief can be detected and avoided. 
% After that, Section~\ref{sec:formal-props} provides changes in the existing problem encoding, formal properties like theorems and their proofs (the proofs omitted due to the page constraints and provided as supplementary), of the new planning scheme. 
% Followed by Section~\ref{sec:eval} discussing empirical evaluation, and showing both qualitative and quantitative results. 
% We then conclude the paper with a discussion.  




The paper\footnotemark is structured as follows: 
Related works are discussed in Section~\ref{sec:related-work}, and Section~\ref{sec:under-framework} briefly describes the underlying framework. 
Section~\ref{sec:model-tom} describes simple ToM models by formalizing SA based on the notion of observability. 
Section~\ref{sec:beliefs-updates} discusses how our proposed models are used to maintain agents' belief and to avoid ambiguities. 
After that, Section~\ref{sec:formal-props} provides changes in the existing problem encoding, and some formal properties of the new planning scheme. 
Followed by Section~\ref{sec:eval} discussing empirical evaluation, and showing both qualitative and quantitative results. 
We then conclude the paper with a discussion.  

%%%% END OF NEW %%%%
% (\textbf{\textit{Attached:}} benchmarks (details), code + a document with detail background, related work, proofs, and more results.)
\footnotetext{\textbf{\textit{Attached:}} benchmarks (details), code + a document with detail background, related work, proofs, and more results.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Underlying
%%      MOTIVATING EXAMPLE   %%
% \section{***Motivating Example***}
% Assume that you (the human agent) want to prepare pasta in your kitchen and want to involve your robot to assist you. This joint task comprises several components and actions needed, e.g., pasta is kept either in the kitchen or the living room, covering a pot, turning a furnace on, the salt container, adding salt to the pasta, etc. 
% Sometimes, some components needed for pasta preparation might be accessible only to you (the robot) from your (its) current position, while some are accessible to both. 

% Suppose the robot has access to your behavioral model. It assumes that you are cooperative, conforming, and rational concerning the joint task/goal but cannot be administered like an artificial agent (i.e., a fully controllable agent). 
% So, if you have several ways to achieve a goal or accomplish a task, the robot cannot dictate one but estimate it non-deterministically. However, it can still act to influence your choices, thus eliciting future actions. Naturally, you would want to work with the robot without being bothered too much; or being lazy to do something, assuming the robot will do it instead and skip, even if it takes longer to achieve the joint task. 
% END of MOTIVATING EXAMPLE %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Here, we 
%Even 
% The above example outlines a crucial aspect of human-robot collaboration: 
% In such scenarios, the robot not only plans for agents' joint tasks but also predicts and emulates human decisions, actions, and reactions to achieve joint tasks seamlessly. 
% Anticipating human decisions based on an accurate estimation of their mental state opens the possibility for creating circumstances to promote actions to be performed by them, achieve the joint task or prevent humans from making errors based on their not up to date or inaccurate beliefs.
% Human Aware Task Planning (HATP) is a framework concerning human-robot collaboration, based on human task modeling and human-aware planning, (partially) tackles these important issues~\cite{alami2006toward,alili2009planning,de2015hatp,lallement2018hatp}.
% A more elaborated architecture, the ``first'' of its type, called HATP/EHDA, extends existing HATP and better \textbf{e}mulates \textbf{h}uman \textbf{d}ecisions and \textbf{a}ctions was recently proposed and showed an improved performance~\cite{BuisanA21,buisan:hal-03684211}.
% This new scheme raises interesting, non-trivial questions. Principled answers to them will improve the proposed HATP/EHDA framework toward task-level autonomy for human-robot collaboration.


% HATP/EHDA comprises a dual-HTNs (Hierarchical Task Networks~\cite{naubooks0014222}), a joint task specification model: 
% Considers a domain modeler available to describe dual-HTNs specifications for collaborative planning. 
% Such models describe agents'  capabilities, initial beliefs, shared tasks, world dynamics, understanding of common ground, etc. Moreover, the modeler implicitly captures hypothetical variables to represent the human mood, intentions, etc., and are non-trivial to manage. Note that both these models are with the robot and it plans for both agents. 
% while the robot plans their joint actions based on the specification models, predicts, and emulates human actions, decisions, and reactions.    
% HATP/EHDA is the state-of-the-art planner~\cite{buisan:hal-03684211}, 
% Without loss of generality, it assumes that agents \textit{decide} to act (e.g., pushing a heavy box, moving, noop, idle, etc.) one after the other. 
% We note that HATP/EHDA is an extension of the HATP framework(s) --- having 
% Note that HATP manages only one search thread for a shared plan, resulting in two coordinated plan streams. 
% But, 
% For planning, a two-threaded search process is proposed, generating a joint plan tree comprising agents' actions. 
% The human agent is uncontrollable, so it is not trivial to determine their ``next'' action upfront, which captures the impact of their mental state, intention,  etc.
% Therefore, for a sound execution, after every robot action: The solution tree branches on all legal ``choices'' available to the human.
 
% However, 
% being in its early stage of development, 
% HATP/EHDA's existing solver makes simplistic \textit{assumptions} during planning like an action executed by an agent would also impact the beliefs of others, i.e., considering the others are always aware of the execution and sharing the same perspective as the former. But, in reality, an agent can execute an action that influences other agents' beliefs differently under different conditions. E.g., \textit{adding} salt or \textit{switching-on} the furnace impacts the human's belief differently depending on whether the human and robot are ``co-present'' (in the kitchen) during execution, or the human assessing the furnace's status post execution.  
% Due to these limitations, the solver often generates plans less robust for real-time execution.        

% We describe a new planning approach that uses enhanced reasoning about the human's and robot's individual beliefs and their evolution. 
% To do so, it utilizes a novel paradigm for implementing suitable ``observability'' based conventions understood during plan execution in the human-robot context based on Theory of Mind (ToM)~\cite{devin2016implemented}. 
% The idea of formalizing such conventions is inspired abstractly by existing situation assessment reasoners performing spatial reasoning and perspective taking during plan execution~\cite{flavell1992perspectives,trafton2005enabling,johnson2005perceptual,Sisbot2011SituationAF,warnier-2012,lemaignan-2017}.
% 
% We provide a general description of this formalization, and to employ it in the HATP/EHDA framework, the existing dual-HTNs formalism is transformed such that the new specifications become less abstract, but enable the new solver to perform enhanced reasoning and to cater to the real-world scenarios that practically are \textit{not possible} to be handled by the existing solver, and to support 
% implicit and explicit 
% communication.

% As a result, 
% Our planner reasons about individual beliefs and their divergences while supporting agents' communication (\text{i.e.}, a decision on ``what'' to convey) and allows for \textit{robust} planning with explicit communication actions utilized ``if'' and ``when'' needed to align agents' beliefs. 
% We evaluate our planner on \textit{three novel domains}, provide qualitative and quantitative analyses, and discuss its theoretical properties.   

% Benchmarks (details), code, and a document detailing the background, related work, and more results are provided.

% In reality, a robot can act that \textit{influences} human's belief differently under different scenarios. For example, adding salt to the pasta or switching on the furnace (in the kitchen) would impact the human's belief state differently depending on whether the human is ``co-present'' in the kitchen at the time of action execution.   

% HATP/EHDA assumes that if the robot executes an action, the human's belief gets updated, along with the robot's belief, or vice versa. It means that all the action execution is observable to both these agents. It is a \textit{major} drawback of the solver, since it is not always the case in reality. For example, in the pasta preparation scenario, suppose the human leaves the kitchen, and before they arrive back in the kitchen, the robot \textit{adds} some salt to the pasta, \textit{lids} the pot, and \textit{turns} on the furnace. 
% Later, when the human arrives in the kitchen: What would be the new belief of the human? Certainly, they will observe that the furnace is {\sc on}, and also the pot is closed. But what about ``there is already some salt in pasta?'' Which creates divergence in the agents' belief states. 
% % In such scenarios, 
% To simplify the reasoning process over multiple task models, we assume that the robot is always aware of the ground truth. That means we ignore the uncertainty associated with what the human does when the robot is absent. 
% And any advancement in this direction is left for future work. 

% In principle, HATP/EHDA can model and specify all the cases with some (ungraceful) modifications at the model specification level, which may arise in human-robot collaborative planning. (The related work section discusses this in detail.)

% can handle different initial distinct beliefs
% To make it more realistic, 
% In this work, 
% We introduce a novel paradigm for implementing \textit{suitable conventions} understood during plan execution in this context. This paradigm helps upgrade the HATP/EHDA's planning system. 
% A high-level idea to formalize it is inspired by existing situation assessment reasoners which perform spatial reasoning and perspective taking~\cite{flavell1992perspectives,trafton2005enabling,johnson2005perceptual,Sisbot2011SituationAF,warnier-2012,lemaignan-2017}. 
% The upgraded planning system handles divergences in human-robot individual beliefs (computed using our proposed model) and plans with explicitly modeled communication actions such that it tackles belief divergences via communication, if and when needed.

% We extend the HATP/EHDA framework. 
% We thank the modeler again for explicitly specifying, which state variables/fluents are \textit{observable} (an persistent effect of an action, e.g., turning on the furnace, which the human can observe with the help of \textit{situation assessment} (SA)~\cite{cite?} -- SA helps the human to assess the environment), \textit{inferable} -- an effect of an action being executed by some other agent, which can only be inferred when the agent sees the action being performed by the other agent, e.g., robot adding some salt to the pasta and human sees it doing so. 
% The latter can be informed about the exact value of the inferable state variables, otherwise, to manage the belief alignment.   
%- both human and robot beliefs will be updated 

% Communication paradigms like text, visual, speech, etc., are prerequisites for building a communication protocol among multiple agents. However, 
% Effective use of a communication modality is essential to achieve the motivation behind their development, say, a seamless collaboration. However, in reality, communication incurs some costs. 
% Moreover, in human-robot interaction (HRI), communicating too much or too little can disturb the overall task achievement process. 
% Hence, deciding ``how'' to communicate is crucial, but it is equally crucial to decide, ``if'', ``when'', and ``what'' to communicate. 
% Assume that a \textit{speech} modality is available while the 
% \textit{decisional} aspect of communication is handled by the new solver with an enhanced reasoning system on the human mental state.
% whether to communicate or not is \textit{decided} by the algorithm. 
% with communication actions.
%

% The paper is structured as follows: We first build a relevant background to understand this framework and discuss the related work. We then discuss the methodology: which includes modeling and planning with communication actions. 
% After that, we present the results and show examples where handling the belief divergences is effective and how the current version of HATP/EHDA might miss some subtleties or even be ineffective under those scenarios. We compare the overall effectiveness of our approach and show that it uses communication when needed in the process. 
% We conclude with a summary, discussion, and future work.

% \pagebreak
\section{Related Work} \label{sec:related-work}
% Our work is at the conjunction of several fields of psychology and task-planning employed in HRC. Related work from all these fields are discussed in this section. 
The section touches on all relevant aspects: I.e., Theory of Mind, particularly, employed in HRI, state-of-the-art planning approaches and models used in similar collaborative settings, and agents' communication, which is critical for collaborative planning. And, since humans' beliefs are modeled and exploited in our work, we also discuss relevant epistemic planning frameworks. We explain where exactly our main idea lies at their intersections.   

\paragraph{Theory of Mind in HRC}
ToM refers to the ability to ascribe distinct mental states to other people and reason about their perceptions~\cite{baron1985does}. 
% Indeed, DA's approach allows the robot to estimate the mental state of the human at runtime and adjust accordingly~\cite{devin2016implemented}. 

HR Interaction (HRI) literature uses ToM in the execution of shared plans. However, the focus shown is on perspective taking --- a robot reasons about what humans can perceive followed by constructing a world from their frame of reference, and hence managing the agents' beliefs accordingly on the fly~\cite{berlin2006perspective,milliez2014framework}.
The framework given by~\citeauthor{devin2016implemented}~(\citeyear{devin2016implemented}) allows the robot to estimate the mental state of the human, containing not only their beliefs but also their actions, goals, and plans. 
It supports the robot's capabilities to do spatial reasoning w.r.t. the humans and track their activities. 
In particular, it manages the execution of shared plans in a collaborative object manipulation context and how a robot can adapt to human decisions and actions and communicate when needed.

In dynamic settings like ours, an agent may believe something true that no longer holds as a ground truth~\cite{DissingB20}, and it must be known to execute the agent's next action (or plan).
The framework proposed in~\cite{ShvoKM22} uses agents' ToM such that agent reasons over the nested beliefs of other agents to handle misconceptions about the validity of their plans and achieves it by communicating with them or by acting in the real world. To realize their idea, the authors relate it to epistemic planning that combines reasoning and planning based on the beliefs and knowledge of agents~\cite{petrick2002knowledge,BolanderA11}.
However, they assume that the agents' plans and (nested) beliefs are given and that the agents are controllable. Certainly, their framework is rich and extendable to cases where agents have possible plans or include a plan recognition technique as in~\cite{CirilloKS09}, and resolving discrepancy based on, for example, the most probable agents' plans.
But in line with ours, their approach also assumes that agents believe that communications are trustworthy~\cite{fabiano2021multi}.
% For example, the robot informs its human teammate that since a certain condition does not hold hence so the success of their plans. The robot can also make the condition hold before human executes.



\paragraph{Planning Approaches, Solution Plans, and Models}
Various task models have been realized in the HR collaborative planning context, e.g., hierarchical task networks (HTNs)~\cite{lallement2018hatp,roncone2017transparent}, POMDPs~\cite{UnhelkarLS19,roncone2017transparent,UnhelkarLS20}, AND/OR graphs~\cite{DarvishSMC21}, etc.
A hierarchical network is created using HTNs (abstract and non-abstract tasks) and AND/OR graphs to represent the inner coupling links of the subtasks~\cite{GombolayJSSS16},
% A hierarchical network is formed using the AND/OR graph and HTNs to represent the inner coupling links of the sub-tasks in~\cite{GombolayJSSS16}, 
and a plan search occurs in a depth-first manner. In~\cite{HorgerKE19}, the authors show how uncertainty can be dealt with in the evolution of the environment and agent behavior. The challenge lies with, especially in POMDPs for HRC, the hidden and implied state of the human agent~\cite{UnhelkarLS20}.

The HATP frameworks extending HTNs consider agents controllable~\cite{alami2006toward,lallement2018hatp}, while in~\cite{roncone2017transparent}, the framework considers planning at multiple abstraction levels (with a single HTN) with humans. 
It is capable of basic reasoning for role assignment and task allocation. Robots plan under state uncertainty with partially observable human intentions modeled and tackled (mainly) at the primitive task level. 
% But these works assume that a joint task is established before planning and building shared plans. 
But these frameworks assume that a joint task is established before planning. Moreover, generally they produce explicitly coordinated, shared HR plans that are legible and acceptable by humans --- they are assumed to be controllable in some sense, such that the techniques said to be relying more on the replanning aspect.
In~\cite{CirilloKS09}, the robot does not plan actions for humans but forecasts their actions/plans from their activities and bases its own decision on the distribution of possible human plans.

Research has been proposed to investigate how to create a reasonable model of human operators and how to obtain task knowledge, e.g.,~\cite{unhelkar2019learning}. 
HR task planning is also modeled as an optimization problem and solved using multiple cutting-edge research, e.g., mixed integer programming~\cite{VatsKL22}. 
Task knowledge can be gathered offline from human psychologists and expert engineers~\cite{levine2014concurrent,wang2018robot,CirilloKS09}; or can be learned via human tutors or from demonstrations~\cite{koppula2016anticipatory}; or a Markov model for sequential decision-making can also be learned from a partial specification of human behaviors~\cite{unhelkar2019learning}. 
% 
Hierarchical models consist of layered abstractions and are considered suitable or close to human intuitions. It helps predict humans' actions or plan predictable behaviors of the robot. 
Such models can be learned using conjugate task graphs, and to identify the task structure an aggregation algorithm can be used~\cite{HayesS16}. 

% We \textit{choose} a task specification based on HTNs for representing a problem compactly. And the most appropriate framework to be extended is HATP/EHDA. 
% We \textit{adapt} its input encoding format and \textit{extend} it to integrate SA models for reasoning and planning (with our new planning scheme).
% However, we note our modeling-level SA proposals could fit in any other planning approaches framing multi-party systems having one controllable agent while can only hypothesize remaining agents' behaviors. E.g., human-centered AI.

% Although we assume that the task models of human and robot are provided as in HATP/EHDA, but will be modified to integrate the agents' SA models.    

\paragraph{Communication in HR Collaboration}
% shashank - can be removed
Communication 
% is a key to successful human-robot collaboration, 
is used to align an agent's belief, clarify its decision or action, fix errors, etc.~\cite{tellex2014asking,sebastiani2017dealing}. 
% We already discussed the ToM-enabled framework by Devin and Alami (2016). The framework handles execution time subtleties such that it decides at the execution time if communication is needed and then the content that should be transmitted. 
% They achieve it by monitoring the divergence between the robot knowledge and the estimated human knowledge. If a belief divergence is detected that can endanger the plan, then verbal communication takes place. 
Recent work deals with an explicit usage of communication actions in planning~\cite{BuisanSA20,nikolaidis2018planning,roncone2017transparent,sanelli2017short,UnhelkarLS20}. 
E.g., in~\cite{roncone2017transparent,UnhelkarLS20}, the authors represent and plan with explicit communication actions, considering them as regular POMDP actions, such that execution policies contain them. 

Redundant communication can be annoying and costly. Due to the invisible state of the human operator, their subsequent action is estimated using, e.g., tracking their \textit{attention}, from   \textit{decision-making} models, and \textit{motion prediction}. 
Next, a POMDP can be created and solved by optimizing the overall benefit/cost as in~\cite{UnhelkarLS20}. 
In this work, 
% following the HATP/EHDA framework, 
we estimate the evolution of the agents' beliefs and decide ``if'' and ``when'' belief alignment is required. And, it is achieved via explicit communication actions, 
% answering ``what" to communicate 
but with minimal communication and in a principled way. Moreover, we do not use these actions for planning (for the deliberation process) explicitly like (non-) primitive tasks.    

\paragraph{Epistemic Planning}
Our notion of the ``observable-fact'' classified into: observable from action and observable from the state, can roughly be seen as a part of the restricted epistemic logic presented and applied in planning applications~\cite{CooperHMMPR21}. 
Our high-level idea of SA is consistent with the concept of perspective shifts in epistemic multi-agent planning -- that extends DEL~\cite{engesser2017cooperative}. 
However, unlike our first-order representation, which is used to maintain agents' distinct beliefs, DEL-based is rich and can model scenarios involving nested perspective-taking.  
In~\cite{KR2021-12}, the concept of perspective shifts is expanded to provide a foundation for producing implicitly coordinated human-robot plans that do not require the agents to negotiate and commit to a joint policy at plan time. 
In specific scenarios, it produces HR policies that are not socially awkward, which is essentially the aim of HRI research. 
However, the work does not consider humans as uncontrollable agents like ours, so, from what we understand, extending their framework to handle the uncontrollability of human operators is not so clear.  

% \paragraph{Explainable AI Planning}
% xyz 

% ****

% ****

% ****

% \textbf{1.} Human task modeling - issues references, what and why, other related issues

% \textbf{2.} Work on HATP: rachid's old work, saffiotti's group, work by Scassellati's group, other task planning methods in HRC

% \textbf{3.} Communication in HRC

% \textbf{4.} ToM - work by Sheila McIlraith's group, Bernhard Nebel's group, and Thomas Bolander's group on collaborative epistemic planning, facts inferred from obs is new and close to [1] not in MAP...

% \textbf{5} Sandra's work on ToM, add examples where that approach is not relevant   -- how sandra's approach with current hatp/ehda plans would behave differently -- fewer replanning/ reducing the dependence on external hardware at run time.

% 6. note that we still need an execution framework like DA's for an end to end execution

% ****

% ****

% Several attempts to model human activities are made such that these models are used to design a system, which integrates into another paradigm dealing with human tasks to improve the latter's performance. 
% A general approach for representing human activity and interaction with a computer at an abstract level is using \textit{task models}. 
% The hierarchical structure of human activity, seeing it at several abstraction levels to meet some criteria, was first exploited by Annett and Duncan~\citeyear{annett1967task}. Later, task modeling evolved to introduce system interactions, and its usage became common in user interface designing processes. Most advanced notations include {\sc ConcurTaskTrees}~\cite{paterno2004concurtasktrees} and {\sc hamsters}~\cite{martinie2019analysing}.
% Such models are used to evaluate interactive systems while inspiring us to choose hierarchical models in this work.
% Such models are used for designing or evaluating interactive systems, provide a task understanding, and inspire this work since they share a high-level architecture with HTNs. 

% Previous approaches generally decompose tasks hierarchically to human-aware task planning and assume a fully controllable and cooperative human willing to achieve a joint-goal together~\cite{alami2006toward,montreuil2007planning,alili2009planning,alili2009task,lallement2014hatp,de2015hatp,lallement2018hatp}. 
% Moreover, the agents are assumed to have established a shared goal before planning. 
% Later, 
% Generated plans are shared with the human before the execution~\cite{milliez2016using}.
% We note that HATP does not represent humans as ``regular'' agents with separate decision processes, which may lead to diverging plans without robot communication.
% Also, it manages only one search thread for a shared plan, resulting in two coordinated plan streams. 

% Other approaches are related to what we do, consider an external human model (e.g., Agent Markov Models (AMMs)~\cite{unhelkar2020decision,UnhelkarLS19}), which predicts human activities, and hence the robot plans accordingly~\cite{hoffman2007effects,unhelkar2020decision,UnhelkarLS19}. Such systems can determine actions that influence human's future actions. While some systems interact with humans even when they are non-collaborating~\cite{buckingham2020robot}.

% In psychology, 
% Spatial reasoning and perspective taking are vital aspects of human collaboration~\cite{flavell1992perspectives,tversky1999speakers}, such that a person can mimic the mind of others to understand their viewpoint. 
% 
% Ideas from psychology got employed in many human-robot contexts. 
% In Robotics, often, ToM focuses on perspective taking and belief management such that a robot reasons out what humans can perceive~\cite{berlin2006perspective,milliez2014framework} and builds representations of the environment from their perspective, which sometimes helps solve ambiguous tasks, predict human behavior, etc.
% \citeauthor{johnson2005perceptual} (\citeyear{johnson2005perceptual}) propose a method based on visual perspective taking for a robot recognizing an action executed by another robot. 
% \citeauthor{milliez2014framework} (\citeyear{milliez2014framework}) propose visual and spatial perspective taking to find {\em the referent} indicated by a human. 
% In~\cite{Sisbot2011SituationAF}, based on spatial reasoning and perspective taking, a reasoner generates online \textit{relations} between agents and objects {\em co-present} in the environment. 
% Such relations are stored in a database and used for either planning, acting, or both.

% To achieve a collaborative task, 
% Indeed, 
% Theory of Mind (ToM) refers to the ability to ascribe distinct mental states to other people and update them by reasoning about their perceptions and goals~\cite{baron1985does}.
% Indeed, the framework given by~\citeauthor{devin2016implemented}~(\citeyear{devin2016implemented}) allows the robot to estimate the mental state of the human, containing not only their belief but also their actions, goals, and plans. It supports the robot's capabilities to do spatial reasoning w.r.t. the human and track their activities. In particular, it manages the execution of shared plans in a collaborative object manipulation context and how a robot can adapt to human decisions and actions and communicate when needed.

% This work is motivated by the main ideas of ToM~\cite{devin2016implemented} and situation assessment based on spatial reasoning and observability while abstractly employing them during planning. 

% Communication 
% is a key to successful human-robot collaboration, 
% is used to align an agent's belief, clarify its decision or action, fix errors, etc.~\cite{tellex2014asking,sebastiani2017dealing}. 
% We already discussed the ToM-enabled framework by Devin and Alami (2016). The framework handles execution time subtleties such that it decides at the execution time if communication is needed and then the content that should be transmitted. 
% They achieve it by monitoring the divergence between the robot knowledge and the estimated human knowledge. If a belief divergence is detected that can endanger the plan, then verbal communication takes place. 
% Recent work deals with an explicit usage of communication actions in planning~\cite{BuisanSA20,nikolaidis2018planning,roncone2017transparent,sanelli2017short,UnhelkarLS20}. 
% E.g., in~\cite{roncone2017transparent,UnhelkarLS20}, the authors represent and plan with explicit communication actions, considering them as regular POMDP actions, such that execution policies generated contain communication actions.

% Following ToM abstractly, our approach estimates the evolution of the agents' beliefs and decides ``if'' and ``when'' belief alignment is required for planning. 
% And, it is achieved via explicit communication actions, answering ``what" to communicate. 
% But, our solver does not use these actions (for the deliberation process) like (non-) primitive tasks.
% but to minimally align an agent's belief with the ground reality 
% so that the belief divergence does not impact the overall task achievement.

% However, this work estimates the evolution of the agents' beliefs and decides ``if'' and ``when'' \textit{belief alignment} is necessary for planning. 
% And, it is achieved via explicit communication actions, answering ``what" to communicate. 
% Our solver does not use these actions (for the deliberation process) like (non-) primitive tasks but to minimally align an agent's belief with the ground reality so that the belief divergence does not impact the overall task achievement. 
% At each stage, if needed, a \textit{sequence} of communication actions is \textit{computed} by a modified Breadth-First Search planning subroutine and \textit{appended} to the sender's plan. We provide clarification on this when we formalize our contributions. 


% **** %%Yet to be updated: END

% ****

\section{The Underlying Architecture} \label{sec:under-framework}
For elaborated basic terminologies, notations, and definitions related to HTNs like task network, problem, and solution, refer to~\cite{naubooks0014222}.  
% \begin{definition}
% \textbf{(HTN Planning Problem.)} 
% The HTN planning
% problem is a 3-tuple $\mathcal{P} = (s_0, w_0, D)$ where $s_0$ is the initial belief state (the ground truth), $w_0$ is the initial task network, and $D$ is the HTN planning domain comprising a set of propositions (P), (non-) primitive tasks (O) and methods.
% \end{definition}

% \begin{definition} \label{def:htn-sol-plan}
% (\textbf{Solution Plan}.) 
% {A sequence of primitive tasks $\pi=(o_1,o_2,o_3...,o_k)$, s.t., $\forall o_i, o_i \in O$, is a solution plan for the HTN problem $\mathcal{P}=(s_0,w_0,D)$ iff there exists a primitive decomposition $w_p$ (for $w_0$), and $\pi$ is an instance of it. 
% }  
% \end{definition}

\subsection{The Planning Framework}
We briefly describe the HATP/EHDA framework and discuss its ability to capture a broad class of HR collaborative planning scenarios. 
It comprises a dual-HTN based task specification model. It plans for the robot to act in the presence of a human agent even when they do not share a task to achieve in the beginning, while it considers the human congruent with the high-level task. 
It can also ask the human for occasional help to accomplish its task or manage the creation of shared tasks, handle human's reactions modeled explicitly via triggers, etc. More details on triggers  in~\cite{ingrand1996prs,AlamiCFGI98}.  

% Following is its brief working description: 
% The basic structure manipulated by the solver is \textit{agent}~\cite{thesisBuisan21}, and as we said earlier, a two-agent model: the \textit{human} and the \textit{robot}, as a dual-HTN is specified. 
% Each model has its own belief state, action model, task network, plan, and triggers. In this work, we are interested in solving the planning problem, $\mathcal{P}_{hr}$, in which agents start with a joint task, i.e., a shared initial task network, $w_0$, to decompose.
% The HATP/EHDA existing planning scheme uses agents' action models and beliefs to decompose the given task network into its legal primitive components. 
% Decomposition updates the current network by inserting new (non) primitive tasks, additional constraints, etc., such that the single-agent process is generalized for the two-agent scenario.
% While doing so, it also updates the belief state of each agent and models their reaction by executing the triggers.
The basic structure manipulated by the solver is two \textit{agent} models~\cite{thesisBuisan21}: the \textit{human} and the \textit{robot}. 
Each model has its own belief state, action model (HTN), task network, plan, and triggers. In this work, we are interested in solving the planning problem, $\mathcal{P}_{hr}$, in which agents start with a joint task, i.e., a shared initial task network, $w_0$, to decompose.
The HATP/EHDA existing planning scheme uses agents' action models and beliefs to decompose the agents' task network into legal primitive components. 
Decomposition updates the current network by inserting new (non) primitive tasks, additional constraints, etc., such that the single-agent process is generalized for the two-agent scenario.
While doing so, it also updates the belief state of each agent and models their reaction by executing the triggers.

Without loss of generality, the planning scheme assumes a single agent decides to act at a time and, also ``which action to execute?'', e.g., add salt, cook pasta, and delay, similarly followed by other agents. 
% In this round-robin fashion (to include a broad spectrum of problems), 
It uses specific actions to synchronize agents' plans. {\em IDLE} is inserted into the agent's plan when its task network is empty, and {\em WAIT} when it does not have regular applicable actions. First, the framework builds the whole search space by considering all possible, feasible decompositions~\cite{buisan:hal-03684211}. 
Then, it can adapt off-the-shelf graph search algorithms, e.g., the well-known algorithms like $A^*$ and $AO^*$, and consider social cost, plan legibility, and acceptability~\cite{alili2009task}, etc., to search for the agents' \textit{joint solution plan}, defined next extends the definition of HTN's solution, for the HATP/EHDA case. 
Note that an implicitly coordinated plan for the robot is contained within such joint plans. 

\begin{definition} \label{def:joint-sol-plan}
(\textbf{Joint Solution Plan}.) 
{The solution for $\mathcal{P}_{hr}$, is represented as a tree or a graph, i.e., $G=(V,E)$. Each vertex ($v \in V$) represents the robot's belief state, starting from initial belief. Each edge ($e \in E$) represents a primitive task that is either a robot's action $o^{r}$, or a human's action $o^{h}$. $G$ gets branched on the possible choices ($o^{h}_1$, $o^{h}_2$, ..., $o^{h}_m$). 
}  
\end{definition}

Each branch, from the root to a leaf node, its sequence of primitive actions, say,  $\pi=(o_1^r,o_2^h,o_3^r,...,o_{k-1}^h,o_k^r)$, must satisfy all the solution conditions. 
Here, each $o_i^h$ represents a choice, often out of several, the human could make.

A simplifying assumption the planning scheme makes is that if the actions of the human and robot at a given time stamp are conflicting, then they need to be modeled and handled explicitly, helping the parallelization to be sound~\cite{CrosbyJR14,ShekharB20}.
On the other hand, the assumption of a single agent acting at a time comes up with an advantage. Since humans are uncontrollable, and so one could think of possible joint actions (the humans' next predicted action could also be \textit{delay} or \textit{leave} among others), this assumption often enables the scheme to explore a broader search space. 
However, most importantly, our main contribution is agnostic to this limitation of the underlying framework.

\subsection{An Understanding of Common Ground}
The formalization below is based on the standard state-variable representation (for more details~\cite{naubooks0014222}) while we abuse the standard notations slightly, sometimes, to maintain the flow of the discussion.

To describe an environment a set of \text{constant symbols} is used. 
Constants are often classified as different groups ($gr$), e.g., \textit{places}, \textit{pots}, \textit{agents}, \textit{etc.} 
Each constant can be represented as an \textit{object symbol}, e.g., \textit{robot1}, \textit{human2}, \textit{pasta-pkt1}, etc. 
An \textit{object variable} belonging to a group can be \textit{instantiated} using any constant symbol of that group.



Suppose $\mathcal{S}$ is the complete state-space and $s \in \mathcal{S}$ represents a single state (a \textit{belief} state in this current context).  

\begin{definition}\label{def:svf}
\textbf{(State Variable Function.)} It is represented as: $f_{svs}:(?g_1 (gr_1), ?g_2 (gr_2), ..., ?g_k (gr_k),\mathcal{S})\rightarrow ?g_{k+1} (gr_{k+1})$. 
\end{definition}
Here, $svs$ is \textit{state-variable symbol} (\textit{attribute} name), while each \textit{term} with ``$?$'' (a question mark), can be instantiated with a \textit{constant} from its respective \textit{group} (mentioned inside `$()$') appears beside the term. 
E.g., to model agents' location in a state, we use $f_{\textit{AgtAt}}:(?a (Agents), \mathcal{S}) \rightarrow ?r (Rooms)$, representing, for each agent and each possible, in a given legal state ($s_i \in \mathcal{S}$), we know where the agent is located in $s_i$. 
Each such possible \textit{instantiation} of defined state variable functions, represents $s_i$ partially, and it is also called a characteristic attribute of $s_i$.     

If a variable $f_{svs_i}: (?g1 (gr_1), \mathcal{S}) \rightarrow ?g_2 (gr_2)$ such that $gr_1$ contains \textit{only} one element in it, then, for a given state $s \in \mathcal{S}$, we simplify this expression to $f_{svs_i}^{s} \rightarrow ?g_2 (gr_2)$. 

Suppose $s \in \mathcal{S}$ is the real state with the ground truth. As per our assumptions, the belief state of the robot w.r.t. the real-world is always the real world state, i.e., $B_{\varphi_r}^s = s$, and the human belief w.r.t. $s$ is $B_{\varphi_h}^s$ --- that is estimated when the robot takes the human's perspective. Each state variable function instantiation with respect to a belief state, $B_{\varphi_r}^s$ ($B_{\varphi_h}^s$), represents the true value of that state attribute with the \textit{perspective} of that particular agent, i.e., $\varphi_r$ ($\varphi_h$). (Both perspectives are managed by the robot in this setting.) 

Suppose that 
% $f_{\textit{svs}_i}:(g_{(1)l},g_{(2)m},...,g_{(k)n},s) \rightarrow g_{(k+1)p}$
% $f_{\textit{svs}_i}:(g_{(1)l},g_{(2)m},...,g_{(k)n},\mathcal{S}) \rightarrow ?g_{k+1} (gr_{k+1})$
$f_{\textit{svs}_i}(g_{(1)l},g_{(2)m},...,g_{(k)n},s) = g_{(k+1)p}$, for a legal state $s$ and $g(i)$ is the $i^{th}$ group while $g(i)j$ is its $j^{th}$ element, represents a possible {\em grounding} (Definition~\ref{def:svf}).

\begin{definition} \label{def:bd}
\textbf{(Belief Divergence.)}
Belief divergence is defined as a situation during planning when there exists an instantiation such that 
% $f_{\textit{svs}_i}:(g_{(1)l},g_{(2)m},...,g_{(k)n},B_{\varphi_r}^s) = {g_{(k+1)p}}  \neq f_{\textit{svs}_i}:(g_{(1)l},g_{(2)m},...,g_{(k)n},B_{\varphi_h}^s)$.
$f_{\textit{svs}_i}(g_{(1)l},g_{(2)m},...,g_{(k)n},B_{\varphi_r}^s) = {g_{(k+1)p}}  \neq f_{\textit{svs}_i}(g_{(1)l},g_{(2)m},...,g_{(k)n},B_{\varphi_h}^s)$.
\end{definition} 

% Note that uncertainty in individual's knowledge is not explicitly modeled, while 
Note that each agent is convinced about their beliefs of the ground truth and considered trustworthy when communicating with others~\cite{fabiano2021multi}.

\section{Modeling Simple Theory of Mind} \label{sec:model-tom}
% We formalize our intended concepts of ToM and use them for planning. 
Agents' SA processes are modeled at the symbolic level and utilized in planning for better anticipation of what can happen and what the agents can know following their estimated sequence of actions during execution. 
% More precisely, These processes are themselves based on a symbolic Place-Based model of observability described below.

% Our main intuition behind formalizing the SA processes is that we want to \textit{exploit} --- by estimating it in a straightforward manner, the agents' real-time sensing ability (or ability to see things, in case of humans) in \textit{offline planning}. We note that they are modeled as \textit{deterministic} processes. 

Our main intuition behind formalizing the SA processes is that we want to \textit{exploit} in \textit{offline planning} --- by estimating in a straightforward manner --- the agents' real-time sensing and reasoning capabilities about their surroundings. 
Note that they are currently modeled as \textit{deterministic} processes. 
% However, we understand that such evaluation is generally done precisely with \textit{geometric reasoners} at run-time, but it would be less intuitive to consider such reasoners while modeling SA at the symbolic level. 
% In line with the DA's framework, those reasoners make more sense during execution and can be used along with perception and supervision to handle robot's behavior to \textit{non-estimated} human initiatives. 
In line with the DA's framework, such evaluation is generally done precisely with \textit{geometric reasoners} at run-time, based on the robot's perception and supervision components, to adapt to \textit{unpredicted} human initiatives. 
However, without available run-time data from perception or a learned model obtained otherwise, it would be less intuitive to consider such reasoners in offline planning.

\subsection{Place-Based Observability}
% We introduce SA models to be able to evaluate the observability of the agents during the planning process. Such evaluation could be done more precisely with some geometric reasoning, but it would be very challenging to use at planning time. 
% This type of reasoning will be done at execution time using perception and supervision to handle unplanned human initiatives. 



% The place-based observability model is abstractly inspired by standard execution time observability conventions relevant for the current context~\cite{devin2016implemented}. 

To make the SA processes more precise and exposition of our intuition easier, we associate them with  \textit{Places} --- which are a particular class of objects in the environment. 

\begin{definition} \label{def:places}
\textbf{(Places.)} Represented as $\mathit{Places}$, captures a group of constant symbols such that each member individually captures a pre-specified area in an environment declared by the domain modeler.  
\end{definition}

% Agents are always ``situated'' in a place or moving between two places. Two agents situated in the same place $p \in \mathit{Places}$, at a given time, are said to be \textit{co-present}.

Note that agents are always either ``situated'' in a place or moving between two places while performing a task. 
% Agents are always ``situated'' in a place. When moving between two places, an agent is considered as situated in both places.

\begin{definition} \label{def:copresence}
\textbf{(Co-presence.)} Two agents situated in the same place $p \in \mathit{Places}$, in a given state $s_i \in \mathcal{S}$, are said to be \textit{co-present} in state $s_i$.
\end{definition}


% \begin{definition} \label{def:pssav}
%     \textbf{(Place Specific State Attribute Function.)} A {\em grounded} state-variable function {\em aka} a state attribute of a given state, can  explicitly be associated with a specific place. More generally, 
%     $f_{loc}: (f_{svs_i}(?g_1(gr_1),...,?g_k(gr_k),\mathcal{S})) \rightarrow Places$
%     % $f_{loc}: (f_{svs_i},\mathcal{S})) \rightarrow Places \cup \emptyset$
% \end{definition}
\begin{definition} \label{def:pssav}
    \textbf{(Place Specific State Attribute Function.)} A {\em grounded} state-variable function {\em aka} a state attribute of a given state is explicitly associated with a place. More generally, 
    % which can be expressed as, 
    $f_{loc}: (f_{svs_i}(?g_1(gr_1),...,?g_k(gr_k),\mathcal{S})) \rightarrow Places \cup \emptyset$.
    % $f_{loc} : (f_{svs_i}(g_{(1)l},...,g_{(k)n},\mathcal{S}), \mathcal{S}) \rightarrow Places \cup \emptyset$.
    % \label{def:pssav}
\end{definition}
% Here, $\mathit{loc}$ is a location specific symbol for {\em place specific attribute}, while the function captures and maintains \textit{place} ($\emptyset$ appears in the definition for the omnipresent attributes also known as static attributes) associated to an attribute $f_{svs_i}(...)$.
Here, $\mathit{loc}$ is a location specific symbol for {\em place specific attribute}, while the function captures and maintains attributes $f_{svs_i}(...)$ associated to a \textit{place}. $\emptyset$ corresponds to omnipresent attributes also known as \textit{static} attributes.


% Such explicit mappings suggest that the attribute is effective 
% (\textit{observable})
% in its dedicated place.
% Such mappings can change when an action changes \textit{place} of the effectiveness of an attribute, e.g., 
% while \textit{holding} a cup the robot \textit{moves} to the adjacent room. 
% These updates are currently manually handled, carefully.
These explicit mappings suggest that the attribute is ``observable'' in its dedicated place. Such mappings can change when an agent's action modifies the \textit{place} of observability of an attribute, e.g., 
while \textit{holding} a cup the robot \textit{moves} to an adjacent room. 
These updates are currently manually handled, carefully.

\begin{definition} \label{def:svof}
\textbf{(State Variable Observability Function.)} 
% (ref to Obesrvability of fluents section)}
The state variable observability function maps each state attribute {\em aka} a grounded \textit{state-variable function} 
to either $\observable$ or $\inferable$
% ; or a more general way to express it is,
% . A more general way to express it is: \newline \indent
. A more general way to express it is:
$f_{\textit{obs}}: (f_{svs_i}(?g_1(gr_1),...,?g_k(gr_k),\mathcal{S}
)) \rightarrow 
    \{\inferable, \observable\}$.
\end{definition}
Here, \textit{obs} represents an \textit{observability} symbol to capture \textit{state-variable observability} such that a state-variable function, for a legal state $s\in\mathcal{S}$, is classified as either \textit{observable} ($\observable$) or \textit{inferable} ($\inferable$). 
We generalize it further, assuming a state-variable function is either \textit{observable} or \textit{inferable}, and hence, relaxing the individual state based restrictions, that means, the above expression can be further simplified to, 
$f_{\textit{obs}}: (f_{svs_i}(?g_1(gr_1),...,?g_k(gr_k))) \rightarrow \{\inferable, \observable\}$.
% $f_{\textit{obs}}: (f_{svs_i}) \rightarrow \{\inferable, \observable\}$.

% When a state-variable function % ($f_{svs_i}$) belongs to the class $\observable$, it indicates that an agent can \textit{assess} the correct knowledge of its \textit{exact} status in a given state $s_l$, if the agent fulfils certain requirements w.r.t. the state $s_l$. 
% But when we say a state-variable function ($f_{svs_j}$) belongs to the class $\inferable$, that means that, unlike the previous case, an agent \textit{cannot} observe it directly. But, under certain scenarios, the agent can definitely predict or infer its \textit{exact} status in $s_l$. 
When a grounded state-variable function ($f_{svs_i}(...)$) belongs to $\observable$, an agent can \textit{assess} the correct knowledge of its \textit{exact} status in a given state $s_l$, if it fulfills certain requirements in $s_l$. 
But, when we say $f_{svs_j}(...)$ belongs to $\inferable$, it means, unlike the previous case, an agent \textit{cannot} observe it directly. 
However, in certain scenarios, the agent can definitely learn or infer its status. 

\subsection{Situation Assessment Processes}
We provide formal definitions for the intended SA processes based on the above \textit{place based observability} description. 
% Based on the Place-Based Observability model we just introduced, we model SA with the following two processes that will be used in the planning process/deliberation process. 

% Our assumption is that the robot is always aware of the ground reality. 
% Hence, technically, the following processes are only effective for the human agent in our case. 

% *** [[WILL MODIFY THE DEF. a bit TOMM.]]
% Observation from a state
\begin{definition} 
\label{def:obs}
    \textbf{(Observation Process.)} When situated in the same place (Def.~\ref{def:pssav}) as an observable (Def.~\ref{def:svof}) attribute, an agent assesses its exact value.  
\end{definition}

% (Based on Definition~\ref{def:pssav}) We can always associate specific state attributes to \textit{places}. 
% For example, suppose that being in the state $s_1$, the robot switches on the furnace placed in \textit{kitchen}, and this generates a new state $s_2$. Also, assume that
% $f_{\textit{TurnOn}}
% $ is $\observable$ (Definition~\ref{def:svof}).   
% Then, the \text{place specific attribute function}, $f_{loc} : (...)$, w.r.t. the states
% $s_1$ and 
% $s_2$ can be expressed as, 
% $f_{\textit{loc}} (f_{\textit{TurnOn}}^{s_1} )$ and $f_{\textit{loc}} (f_{\textit{TurnOn}}^{s_2} )$, respectively, and both map to $\textit{kitchen} \in Places$.

% Consider the following scenario: The search progresses from $s_2$, along $s_1, s_2, ...$, such that the next action applicable in it is, the agent $(\varphi_h)$ \textit{moving} to the kitchen. This generates a new state $s_3$, and hence $f_{\textit{loc}} (f_{\textit{AgtAt}}(\varphi_h, s_3) )$ maps to $kitchen$, but so does  $f_{\textit{loc}} (f_{\textit{TurnOn}}^{s_3})$. 
% In such cases, $\varphi_h$ assesses the \textit{status} of the furnace, i.e., the exact value of $f_{\textit{TurnOn}}^{s_3}$ in $s_3$, which is {\sc on}, and hence the human agent updates their belief.  

The \textit{Observation} process models the ability of agents to observe their surroundings (modeled as $place$) and 
% update their belief based of
learn about the observed facts. 
Hence, through this process, an agent can learn about the \textit{observable} effect of an action performed earlier by others. 

% Here, the robot takes the human's perspective and performs spatial reasoning as per the human's frame of reference or their current location in the environment. 
% The human's belief is updated w.r.t. what they can see as ground truth (mimicked by the robot).
% This enables $\varphi_h$ to learn an \textit{observable} attribute's value achieved earlier by the robot.

% Observable from an action
\begin{definition} \label{def:inf}
    \textbf{(Inference Process.)} When observing an action being executed, either by itself or by being co-present with the acting agent throughout its execution, an agent can infer the new values of the affected inferable attributes.  
\end{definition}

% \textbf{(\textit{Action Ob    servability}.)}
% An action executed by an agent is \textit{observed} by other agents \emph{co-present} throughout the action execution. Therefore, we state that if an agent observes an action getting executed, the agent will infer the \textit{inferable} effects of the action.
% Consequently, the agent immediately updates its belief state while each inferable attribute affected by the action receives a new value.  

The \textit{Inference} process models agents' ability to reason about, and deduce non-observable facts in their surrounding under certain conditions. Thus, actions including inferable effects that being non-observed create a belief divergence and cannot be fixed like the former case.

% *** * *** %%%

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/cartoon_obs(2).png}
    \caption{
    While Human is grabbing a spoon in the kitchen, Robot adds sugar to the cup before pushing it forward. \textit{Assessing} the new situation after the two actions, Human can only ``know'' the cup's new position (\textit{observable}), but not \textit{SugarInCup} (\textit{inferable}). Human would know about the sugar if they observed Robot acting.
    }
    \label{fig:cartoon}
\end{figure}

% To better understand these observability classes, consider a scenario with a cup of coffee and some sugar on the table as depicted in Figure~\ref{fig:cartoon}. 
% Initially, the cup is far from the human and without sugar. While the human is not attentive (i.e., not \textit{co-present}, say) to the scenario, the robot adds some sugar to the cup and pushes it towards the human. 
% Once the human is attentive again, they
% can notice that the cup is closer. We can say that the position of the cup is \textit{observable}.
% However, without observing the robot adding sugar, the human cannot know if some sugar is added to the cup. Hence, we say the \textit{SugarInCup} attribute is \textit{inferable}, 
% and the human agent would know if they observed the robot adding it. 

% To better understand the proposed place-based observability model and its associated SA processes, 
% Let 
To better understand these models, let
us consider a scenario in which a cup of coffee and some sugar in a bowl are kept on the table, as depicted in Figure~\ref{fig:cartoon}. The joint goal is that the human is holding a spoon and a cup of coffee with some sugar added. 
Suppose that the cup is initially far from the human and without sugar. 
The robot estimates that the human will go to the kitchen to grab a spoon and come back. So, they will not be co-present for some moment. 
Meanwhile, the robot plans to add some sugar to the coffee before pushing the cup toward the human side of the table.
When the human is back, they will notice that the cup is at an accessible distance since it is an \textit{observable} fact (assessed using the \textit{Observation} process). 
But, just by observing the scene (i.e., the current state), the human will not be able to \textit{assess} whether there is already some sugar added to the coffee (an \textit{inferable} fact). 
% Suppose if the human agent is lazy and does not leave the scene as estimated by the robot, then she would assess the robot's action \textit{adding sugar}, and hence would infer that sugar is added (\textit{Observable From an Action}). 
Suppose a spoon was already present on the table, the human would have no reason to leave, and so they would observe the robot's action and infer that sugar is added (\textit{Inference} process).


\section{Belief Updates} \label{sec:beliefs-updates}
There are multiple ways an agent's belief can get updated during planning. These update methods can either be part of the nominal action workflow (next) or a result of handling detected ambiguous situations, which is explained later in this section. Let us first consider the former case.

\subsection{Nominal Action Workflow}
During the deliberation process, the agents' beliefs are mainly updated by a workflow that is based on the computed next action that is appended in the partial plan.
% (starting with a \textit{dummy} action in the initial state). 
% After each action, the workflow for updating agents' beliefs is the following.
After each action, the following workflow updates the agents' beliefs:

\begin{enumerate}
    \item \textbf{(Acting.)} The acting agent's belief is updated with the action effect. Each state attribute appearing in the effect gets updated with its corresponding new value, while the remaining attributes keep their same old values.
    \item \textbf{(Inference.)} \textit{Co-present} agents update their belief with all the \textit{inferable} effects of the action.
    \item \textbf{(Observation.)} All agents update their belief states with their respective \textit{observable} facts.
    % in the next state. 
\end{enumerate}

% To update observable facts present initially using the \textit{Observation} process, first, a dummy action is executed with no precondition and effect.
Agents' plan always start with a dummy action with no precondition and effect such that the \textit{Observation} process is executed and updates initial observable facts.


Note that we do not yet consider cases where the robot's beliefs can diverge, too, assuming that, while not being observed, humans only make deterministic moves. Hence, regardless of being co-present, the robot always updates its belief with both the \textit{inferable} and \textit{observable} effects.

\subsection{Handling Ambiguous Situations}
In the deliberation process, when estimating the humans' next action(s), we check whether there is an \textit{ambiguous situation} caused by estimated \textit{false} human beliefs. 
% If so, the ambiguities are identified and then fixed. We propose two ways to fix such an ambiguity. 
If so, the ambiguities are identified and then fixed by either of the two ways we propose.
The first is through minimal communication, which is a formally proven robust and safe method. The second one is through delayed robot actions, which is more conceptual as we have only a pilot study available. But it illustrates the proactive behavior it can provide to the robot.

Note that even though we do not explicitly model uncertainties in what humans believe, we deal with them nonetheless. 
Indeed, when such uncertainties arise --- especially when the robot acts while the human is not co-present --- we assume that humans would think that nothing has changed until they really \textit{observe} a change. 
A rational human might not use such reasoning, but without absolute trust in the robot, a human would likely be unsure about the robot's actions' effects that are non-observable. 
Due to this, humans could show unpredictable behaviors at execution time which might be superfluous, or even detrimental to the joint task (e.g., trying to achieve a subtask the robot has achieved).
To improve the interaction and the execution of the joint plan, we actively seek to identify and avoid this kind of ambiguity. And to do this, we consider the scenario in which there are no uncertainties and humans are unaware of non-observable changes, which leads to false beliefs.
% This is the kind of ambiguity we are trying to detect and avoid proactively during planning to improve the interaction and the execution of the joint-plan. And to do so, we consider the worst case where there is no uncertainties and the human isn't aware of non-observed changes (has false beliefs).

% \subsubsection{(Detection.)}
\subsubsection{Detection}

% OLD
% The human and robot agents carry individual, distinct belief states w.r.t. the ground truth ($s$) (s.t. $\mathit{B}_{\varphi_r}^s$ is aligned with it), while the two can be either different or aligned. 
% However, the agents act as if they are certain about their knowledge.
% Since the robot plans for both, it reasons about their distinct beliefs and manages the evolution of their individual beliefs. E.g., the robot adding salt to the pasta while the human being co-present or not can grow their belief states differently. 

% To produce a legal joint-plan, the robot is fine with such a divergence unless it has an \textit{adverse} effect, e.g., the human acting based on their false belief.
% At a stage, if the human agent, based on $\mathit{B}_{\varphi_h}^{s_i}$, can perform a set of actions that differs from what they could perform w.r.t. to $\mathit{B}_{\varphi_r}^{s_i}$ (or the ground reality), then we say that such divergent beliefs are \textit{relevant} to be \textit{tackled}.
% sA divergence is also declared as \textit{relevant} if an action has different effects w.r.t. the other agent's belief.

% One approach to tackle it could be to trace back to the action that creates this divergence and {\em DELAY} it for future, if and when possible. E.g., the robot delays adding salt until the human arrives in the kitchen. 
% Second, communicating relevant divergences to the human agent, and it is our main focus in this work. (However, delaying may not always find a solution, but it could reduce communication requirements. Our pilot development and results are in \textit{Appendix~C}.)

% Assessing the impact of different human actions (based on their wrong $\mathit{B}_{\varphi_h}^{s_i}$) or effects, with their overall positive and detrimental impacts on achieving a joint task can be interesting.
% An approach can be to analyze the agents' plan history or future actions to decide whether their beliefs should get aligned or not, or for that matter, how much to communicate, but it is out of the scope of this work. 

% Anthony-V1
% The human and robot agents carry individual, distinct belief states w.r.t. the ground truth ($s$) (s.t. $\mathit{B}_{\varphi_r}^s$ is aligned with it), while the two can be either aligned or diverging. However, how explained above, the agents act as if they are certain about their knowledge.

% To produce a legal joint-plan, the robot is fine with such diverging beliefs unless it has an \textit{adverse} effect, e.g., the human acting based on their false belief.
% Hence, we declare a belief divergence as \textit{relevant} if the human agent, based on $\mathit{B}_{\varphi_h}^{s_i}$, can perform a set of actions which is different or with different effects than w.r.t. to $\mathit{B}_{\varphi_r}^{s_i}$.

% Currently, as soon as a belief divergence affects the next human actions it is considered as \textit{relevant} and needs to be tackled. Checking the relevance of a divergence could be done more accurately by assessing its exact overall positive and detrimental impacts on achieving the joint task but it is out of the scope of this work. 

% Anthony-V2
The human and the robot carry individual, distinct belief states w.r.t. the ground truth,
% ($s$) (s.t. the robot belief state $\mathit{B}_{\varphi_r}^s$ is aligned with it),
while the two beliefs can be either aligned or diverging. 
% However, as we discussed, the agents act as if they are certain about their world knowledge.
To produce a legal joint-plan, the robot is fine with such belief divergences unless they are qualified as \textit{relevant} (Definition~\ref{def:reldiv}). 
In such case, the relevant divergences need to be tackled.

\begin{definition} \label{def:reldiv}
\textbf{(Relevant Belief Divergence.)} A relevant belief divergence, also referred to as an ambiguous situation or ambiguity, is a belief divergence that influences the next human action(s). In other words, if the human agent, based on $\mathit{B}_{\varphi_h}^{s_i}$, can perform a set of actions which is different or with different effects than w.r.t. to $\mathit{B}_{\varphi_r}^{s_i}$.
\end{definition}

% \textbf{[[]Note also that }the appearance of a relevant divergence and when it is effective (i.e. when it influences the next human actions) may be different. 
The appearance of a diverging attribute in the belief state may not make it immediately \textit{relevant}. 
% it becomes relevant 
% divergence 
% (i.e. it influences the next human actions) 
% might be different.
For example, a diverging attribute may be present in the initial human belief, but it can be predicted to be affecting the humans' actions only in a later stage of planning.
This affects \textit{when} will the robot communicate with the human.
% (discussed later). 

% Currently, as soon as a belief divergence affects the next human actions, it is considered as \textit{relevant} and needs to be tackled. 
% Although we keep the notion of relevancy quite simple in this context, 
However, it could be an interesting future work to check in a principled way, the relevance of a belief divergence 
% divergence could be done more accurately, which could be 
by assessing its overall positive and detrimental impacts on the joint task by allowing humans to act with their wrong beliefs and not communicating immediately.
% , in a principled way. 
But, it is out of the scope of this work. 

\subsubsection{Resolved with Minimal Communication}

% Its belief gets updated by learning ground truth when another agent communicates an attribute-value pair. Communication occurs via a \textit{distinct set} of actions, modeled as a predefined communication protocol for a pair of sender-receiver agents. 

% As the robot is always aware of ground truth, again, technically, it is effective only for the human belief.
% We note that each action communicates only {\em one} attribute-value pair, so just one attribute gets updated in the human belief.

% We establish protocols between each \textit{sender-receiver} agents pair for communication, 
% and model explicit communication actions between them. 
% Note that these actions are different than agents' (non-) primitive actions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% When a relevant belief divergence is detected, it can be avoided by planning communication actions s.t. the robot aligns only the required facts in the human belief.

A belief state marked as \textit{relevant} can be tackled by planning communication actions such that the robot communicates/aligns only the required facts in the human belief.
The communication occurs via a \textit{distinct set} of communication actions, which are modeled as a predefined communication protocol for a pair of sender-receiver agents.

% \paragraph{Modeling Communication Actions} 

% Suppose there exists a state-variable function, $f_{svs}:(?g_1 (gr_1), ?g_2 (gr_2), ..., ?g_k (gr_k),\mathcal{S}) \rightarrow ?g_{k+1} (gr_{k+1})$ such that,
% for the current world state $s \in \mathcal{S}$, the corresponding state attribute, $f_{\textit{svs}}(g_{(1)l},g_{(2)m},...,g_{(k)n},s)$ maps to $q$.

% An agent $\varphi_i$ can communicate this attribute-value pair to another agent $\varphi_j$, if the following conditions meet as action precondition: 
% (1.) For $\varphi_i$, its belief state satisfies the fact being communicated, i.e.,
% $f_{\textit{svs}}(g_{(1)l},g_{(2)m},...,g_{(k)n},B_{\varphi_i}^s) = q$, and (2.) $\varphi_j$'s belief state does not satisfy this truth, i.e., $f_{\textit{svs}}(g_{(1)l},g_{(2)m},...,g_{(k)n},B_{\varphi_j}^s) = r$ such that $r \neq q$.
% The effect of this action is that now 
% $\varphi_j$'s belief is updated, i.e., $f_{\textit{svs}}(g_{(1)l},g_{(2)m},...,g_{(k)n},B_{\varphi_j}^s) \leftarrow q$. 

% At this stage, say, the ground truth ($s_i$), we assume \textit{Updates Workflow} was already called, and then the sender reasoned out and decided to \textit{tackle} the receiver's divergent beliefs. 

%%%%%%%%%%%%%%%%%%%

\noindent \textit{\textbf{(Modeling Communication Actions.)}} 
% Suppose there exists a state-variable function, $f_{svs}:(?g_1 (gr_1), ..., ?g_k (gr_k),\mathcal{S}) \rightarrow ?g_{k+1} (gr_{k+1})$ such that,
% for the current world state $s \in \mathcal{S}$, the corresponding state attribute, $f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},s)$ maps to $q$.
% 
% An agent $\varphi_i$ can communicate this attribute-value pair to another agent $\varphi_j$, if the following conditions meet as action precondition: 
% (1.) For $\varphi_i$, its belief state satisfies the fact being communicated, i.e.,
% $f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},B_{\varphi_i}^s) = q$, and (2.) $\varphi_j$'s belief state does not satisfy this truth, i.e., $f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},B_{\varphi_j}^s) = r$ such that $r \neq q$.
% The effect of this action is that now 
% $\varphi_j$'s belief is updated, i.e., $f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},B_{\varphi_j}^s) \leftarrow q$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We propose a generic communication action schema ($ca$) in this context. 
Suppose there exists a state-variable function,  $f_{svs}:(?g_1 (gr_1),..., ?g_k (gr_k),\mathcal{S}) \rightarrow ?g_{k+1} (gr_{k+1})$, s.t. for the current world state $s \in \mathcal{S}$, 
% $f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},s)$ maps to $q$.
$f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},s) = q$.

The agent $\varphi_i$ can \textit{communicate} 
this attribute-value pair to $\varphi_j$ \textit{via} the action $ca_{\varphi_i, \varphi_j}(f_{\textit{svs}}(...),q)$, if the following conditions (i.e., its preconditions) meet: 
(1) For $\varphi_i$, 
$f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},B_{\varphi_i}^s) = q$, and (2) for $\varphi_j$,
$f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},B_{\varphi_j}^s) = r$ s.t.
$r \neq q$.
Later, $\varphi_j$'s belief is updated, i.e., $f_{\textit{svs}}(g_{(1)l},...,g_{(k)n},B_{\varphi_j}^s) \leftarrow q$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% At this stage, say, the ground truth ($s_i$), we assume \textit{Updates Workflow} was already called, and then the sender reasoned out and decided to \textit{tackle} the receiver's divergent beliefs. 

% \subsubsection{Communicate Only The Required Facts.}
\noindent \textit{\textbf{(Communicate Only The Required Facts.)}}
Note that there exists at least one \textit{attribute}-\textit{value} in the receiver's belief state causing adverse effects. 
The approach, explained in the next \textit{three} steps, looks for the minimal number of such attribute-value pairs to be communicated so that
the divergence in the updated belief is made \textit{irrelevant}, or even eliminated.
\begin{enumerate}
    \item 
    \textit{Store} each attribute and its value if the attribute's value differs in the receiver's belief from the sender's belief. 

    % \item For them, \textit{build} actions, $\textit{comm-action}_i$, following the schema described earlier. Say, they are equally costly. 
    \item For each stored attribute-value pair, \textit{build} a communication action $ca_{\varphi_r, \varphi_h}(...)$ based on the schema described earlier. They are considered equally costly.
    
    \item 
    % (Follow Breadth-First Search) 
    % The \textit{source} is $B_{\varphi_h}^{s_i}$, and each $\textit{comm-action}_i$ changes and aligns \textit{exactly} one attribute while its updated value satisfies the ground truth. If $\textit{comm-action}_i$ is applied, generates a new belief state (following the regular state transition rules), using: $B_{\varphi_h}^{s_i,1} = \gamma(B_{\varphi_h}^{s_i}, \textit{comm-action}_i)$.
    % It continues until the first (updated) belief is selected to expand s.t. its remaining divergence is \textit{ineffective}. We then \textit{retrieve} all the actions used from the root until the current belief state.   
    \textit{(Breadth-First Search.)} 
    The \textit{source} is $B_{\varphi_h}^{s_i}$, and each $ca_{\varphi_r, \varphi_h}(...)$ changes, aligns \textit{exactly} one attribute and its updated value satisfies the ground truth. 
    Applying $ca_{\varphi_r, \varphi_h}(...)$ generates a new state following state transition rules, i.e., $B_{\varphi_h}^{s_i,1} = \gamma(B_{\varphi_h}^{s_i}, ca_{\varphi_r, \varphi_h}(...))$.
    Continue until the first updated belief is \textit{selected} to expand s.t. its remaining divergence is ineffective. The actions used from the root until the current belief state are \textit{retrieved}.
\end{enumerate}

% These retrieved actions are appended to the sender's plan.
% Once the above subroutine finishes, the retrieved action set $\mathit{CA} = \{ca^{i}_{\varphi_r,\varphi_h}(...)\}$ is utilized for belief alignment. 
% We redefine Definition~\ref{def:joint-sol-plan} to make it sound w.r.t. our new planning approach. 
% Assume at each step ($t=0,1,2,...$), humans perform SA, while the robot executes each communication action $ca \in \mathit{CA}$, such that the human's belief state \textit{updates immediately} (takes $0$ seconds). 
Once the above subroutine finishes, the retrieved action set $\mathit{CA} = \{ca^{i}_{\varphi_r,\varphi_h}(...)\}$ must be inserted in the plan for belief alignment. Thus, Definition~\ref{def:joint-sol-plan} is redefined to be sound w.r.t. our approach. 
At each step,
% ($t=0,1,2,...$)
humans perform \textit{Observation}, while the robot executes each communication action $ca \in \mathit{CA}$, s.t. the human's belief \textit{updates immediately}.
% (takes $0$ seconds). 

% * Maybe say that currently $\mathit{CA}$ is inserted in the closest step in the plan before the relevant divergence. But it could be interesting to find the optimal prior step to insert it.* 
Currently, $\mathit{CA}$ is inserted in the plan just before the next human actions that would differ without belief alignment. But it could be interesting to find, among all previous steps, the optimal time to communicate.  


\subsubsection{Resolved by Delaying a Robot Action}
% *
% Another way to avoid an ambiguous situation (fix the relevant belief divergence) is by delaying a robot action. This solution is only discussed qualitatively in this section since it has some limitations (not always feasable:rel div due to only one INF attr, HR must be copresent again. Thus usage is based on plan evaluation, compare COM and DELAY, but accurate plan evaluation is still lacking)
% However, this solution illustrate one of the possible proactive behavior the robot could have thanks to our contribution, thus is worth discussing and mentioning.
% *

% Encountered relevant belief divergences are created mainly due to the missed inferable effects. 
% Thus, another way to avoid such ambiguity is to identify the planned robot action creating the relevant divergence and postpone it until the human is \textit{co-present} s.t. the \textit{Inference} process can applied and make the originally missed effects known by the human.
% Thus, another way to avoid such ambiguities is to postpone the planned robot actions that are not observed by the humans, this is in order to make the originally missed effects known to the human. 
% The corresponding process has one strong assumption that is the relevant divergence must come from only one inferable attribute. 
% With this assumption, we propose the following: 
% % The process is the following:
% First, the relevant divergence is solved with communication to create a safe branch in the solution plan tree. 
% Then we check if the relevant divergence was already present in the initial human belief or if it is due to a non-observed action, we proceed only in the latter case. 
% Next, we identify the diverging action by sequentially regressing the current trace.
% % while monitoring when the relevant inferable attribute diverges. 
% Once identified, a new branch is created before this diverging action, and {\sc delay} actions are inserted in the robot's plan until the human is \textit{co-present}. 
% Then, the diverging action is inserted and the nominal planning process is resumed.

Encountered relevant belief divergences are created mainly due to the missed inferable effects. 
Thus, such ambiguities can be avoided by postponing the right non-observed planned robot actions in order to make the human align their belief through perception. 
% Thus, another way to avoid such ambiguities is to postpone the planned robot actions that are not observed by the humans, this is in order to make the originally missed effects known to the human. 
The process is the following:
First, the relevant divergence is solved with communication to create a safe branch in the solution plan tree. 
Then we check if the relevant divergence was already present in the initial human belief or if it is due to a non-observed action, we proceed only in the latter case. 
Next, we identify the diverging action by sequentially regressing the current trace.
% while monitoring when the relevant inferable attribute diverges. 
Once identified, a new branch is created before this diverging action, and {\sc delay} actions are inserted in the robot's plan until the human is \textit{co-present}. 
Then, the diverging action is inserted and the nominal planning process is resumed.


% Algo:
% \begin{itemize}
%     \item Preconditions: a rel div has been detected and identified, and it is on only one INF attr.
%     \item Solve the rel div with com (create one safe branch)
%     \item Check if the div is due to a non observed action or initial divergence (if so, COM needed)
%     \item Find the corresponding non observed action (diverging action)
%     \begin{itemize}
%         \item[] (a) Apply sequentially actions of the plan from initial state and monitor the rel Inf attribute 
%     \end{itemize}
%     \item Create a branch in solution tree before non obs R action and replace it with R delay actions until co-present with H (if never co-present, fails and delete the branch)
% \end{itemize}

% \begin{algorithm}
% \caption{Fixing ambiguity with delayed robot action}
% \begin{algorithmic}[1]

% \State Apply Comm. Solution \Comment{Creates one safe branch}
% \IF{}

% \EndIf

% \end{algorithmic}
% \end{algorithm}


% This delaying solution is only discussed qualitatively in this section because it currently has limitations compared to the communication one. Indeed, the relevant divergence must be due to only one \textit{inferable} attribute and the agents must co-present at least once after the diverging action. Thus, delaying the robot action to fix the divergence isn't always feasible. Therefore, this solution is used along with the communication solution, which is always working, and we later select the best one to use. However, this selection is based on accurate \textit{plan evaluation} which is currently lacking in the scheme and out of the scope of this paper.  

% To use this approach, the relevant divergence must be due to only one \textit{inferable} attribute and the agents must be co-present at least once after the state in which the diverging action was applied. 
% Thus, delaying the intended robot's action is not always feasible. 
% The approach is used along with the communication solution, such that the latter is always working, and we later can select the best one for the robot. 
% However, this selection is based on an accurate \textit{policy evaluation} method, which is currently lacking in this planning scheme.
% Due to all these limitations, this solution approach is only discussed
% % qualitatively
% in this section and will not appear in the formal proofs and evaluations parts. 
% Nevertheless, we see it worth mentioning and discussing it briefly as it illustrates well the possible proactive behaviors the robot can be endowed with --- thanks to our contributions. 
% More details and preliminary results are provided in the appendix.

To use this approach, the relevant divergence must be due to only one \textit{inferable} attribute and the agents must be co-present at least once after the robot starts ``delaying". 
Hence, this solution isn't always feasible and it has to be used along with the communication solution, which is always working.  We later select the best one for the robot. 
However, this selection is based on an accurate \textit{policy evaluation} method, which is currently lacking in this planning scheme.
Due to all these limitations, this solution is only discussed in this section and will not appear in the formal proofs and evaluations parts. 
Nevertheless, we see it worth mentioning and discussing it briefly as it illustrates well the possible proactive behaviors the robot can be endowed with --- thanks to our contributions. 
More details and preliminary results are provided in the appendix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Formal Properties} \label{sec:formal-props}
Note that our solver has a similar high-level elaboration process, i.e., its underlying mechanism, as the existing solver but with a better estimation of the evolution of agents' beliefs.  
However, when discussing its soundness and completeness, dependent on the soundness and completeness of the underlying mechanism, we must distinguish between the problem specifications used by these solvers. 

In the worst case, our specifications would pick each proposition $p \in P$, and generate a new set of primitive propositions for every possible combination of $|\{\observable,\inferable\}| \times |\mathit{Places}|$. 
So, the new encoding size for primitive propositions is worst-case \textit{linear} in the size of $P$.

For the old specifications and to support the existing solver's simplified assumptions, an equivalent encoding to the above can be considered $p$ belonging to the $\inferable$ class for every $place \in \mathit{Places}$, and the agents can \textit{always} be considered co-present.  
Note that for a domain like Blocksworld where explicit modeling of $\mathit{Places}$ is not required, it considers a \textit{single} dummy place $p_{du}$ (depicting that everything occurs here) to serve this new encoding. So, it \textit{ensures} that transformed formalism captures everything that can be formulated using the original dual-HTN but technically provides a latitude to model more realistic problems. 

%%%%%%%%%% THEOREMS %%%%%%%%%%%%%
% \begin{theorem}
% \textbf{(Soundness.)} The new solver is sound.
% \vspace{-0.06in}
% \begin{proof}
% The proof appears in Appendix C.
% % Following Definition~\ref{def:joint-sol-plan},
% % we show that the joint solution plan generated is executable:
% % For a branch, it is guaranteed that it meets the {\em solution conditions}.
% % Moreover, each precondition for an agent's action is achieved in earlier time stamps by either: its own action, 
% % \textit{inferred} while observing another agent's actions, 
% % another agent \textit{communicated},
% % or it is \textit{assessed} by observing a situation and reasoning.
% % Of course, another agent’s action supplying the precondition, an attribute (in $\observable$) and its value, cannot be destroyed. 
% % Hence this branch/trace is executable.

% % Finally, since the human can make any choices during execution,
% % and are unknown upfront, any of its branches, by the
% % above argument, will be executed. 
% % Implies, the joint solution plan is executable.
% \end{proof}
% \end{theorem}

% \begin{theorem}
% \textbf{(Completeness.)} The new planning algorithm is complete, provided the underlying mechanism can exhaustively generate all possible plan elaborations. 
% \vspace{-0.06in}
% \begin{proof}
% The proof appears in Appendix C.
% % Suppose a problem modeled based on our formalism has a solution, say, a joint solution tree, $\tau$ and is sound. 
% % From $\tau$, remove all the communication and situation assessment steps, and say it generates $\tau'$. Now, relax the original problem, considering all the agents are always co-present and making all the propositions \textit{inferable} everywhere. 
% % Technically, the solver will generate $\tau'$ for this new problem when it exhaustively generates the whole search space (of course, it may contain redundant actions). 
% % We assure it believing in the underlying mechanism and that there is at least one solution ($\tau$) for the original problem. As a result, it is trivial to visualize that $\tau'$ is always extendable to generate $\tau$ w.r.t. the original problem. 
% % % (\textit{Updates Workflow}.) 
% % After the execution of each step, one can employ \textit{Nominal Action Workflow}, checking for belief divergences and using communication actions if needed. 
% \end{proof}
% \end{theorem}

%%%%%%%%%% END THEOREMS %%%%%%%%%%%%%

\begin{theorem}
\textbf{(Soundness and Completeness.)} The new planning algorithm is sound and complete.
\vspace{-0.06in}
\begin{proof}
Their individual proofs appear in Appendix C.
\end{proof}
\end{theorem}
    

\section{Evaluation} \label{sec:eval}
% \textbf{** related work section suggests - no existing planner available to compare against -- adjust this statement here clearly***}
% We discussed in related work section similar approaches but 
From what is discussed in the related work section, we are not aware of a planning system (which is implemented) that can be used as a baseline to compare our planner, apart from the HATP/EHDA existing solver. We compare these two planners on three \textit{novel} planning domains.

However, before we describe the domains used in the experiments and the results to compare the existing solver and our new approach, let us make a ``high-level'' distinction between the two approaches. 
% In principle, the old solver can handle a belief divergence only when agents start with unaligned, distinct beliefs. But, when it is not so, unlike ours, it assumes that the agents' beliefs never diverge during planning, and hence there is no way it can handle a divergence created by an agent in practice. 
% The old solver can align the beliefs via explicitly influencing (the human's) action model and a cumbersome technique using triggers to update their task networks. Our approach estimates the evolution of belief divergences at planning time and uses explicit communication actions to handle them in practice to generate robust plans. 
The existing solver, unlike our approach, assumes that the agents' beliefs never diverge during planning, and hence has no way to handle a divergence created by an agent in practice. However, in principle, cases where agents start with unaligned beliefs can be handled. Nonetheless, the action model of the agents must be explicitly designed to handle such divergences since they are not dealt within the solver itself. Hence, it is through cumbersome and domain-dependent modeling techniques that initial belief divergences can be handled with the existing solver.
Whereas our approach estimates the evolution of belief divergences during planning and handles them in a principled way with planned communication actions.

\subsubsection{Cooking Pasta Domain:}
Suppose a stove and salt are available in \textit{Kitchen} $\in$ $\textit{Places}$, and the pasta can be either in \textit{Kitchen} or \textit{Room} (the two are adjacent). The agents have different roles and can only operate in the two places. Robot \textit{adds} salt to the pot and \textit{turns-on} the stove. Human \textit{grabs} the pasta and \textit{pours} it into the pot. 
Pasta can be poured after salt is added to the pot and the stove is {\sc on}.
Focus on the following two attributes associated to \textit{Kitchen}: For a given state, $s_i \in \mathcal{S}$, $f_{\textit{SaltInPot}}^{s_i} \in \{\textit{true, false}\}$ and $f_{\textit{stove}}^{s_i} \in \{\textit{on, off}\}$ such that only $f_{\textit{SaltInPot}}^{s_i}$ is \textit{inferable}, all others belong to $\observable$.

\subsubsection{Preparing Box Domain:}
A box with a sticker on it and filled with a fixed number of balls is considered prepared and needs to be sent. Both agents can \textit{fill} the box with balls from a bucket, while only the robot can \textit{paste} a sticker and only the human can \textit{send} the box. The bucket can run out of balls, so when one ball is left, the human \textit{moves} to another room to \textit{grab} more balls and \textit{refill} it. 
The number of balls in the box is \textit{inferable}, while all other variables are {\em observable}. 

\subsubsection{Car Maintenance Domain:}
The washer fluid ($\observable$) and engine oil ($\inferable$) levels have to be \textit{full} before \textit{storing} the oil gallon in the cabinet ($\inferable$). 
Only the robot can \textit{refill} both the tanks and store the gallon while situated at \textit{Front} of the car. 
\textit{Front-left} and \textit{Front-right} headlights have to be \textit{checked} and a light-bulb has to be \textit{replaced} at \textit{Rear}. 
Only the human can check and replace lights, and they can start with either of these two tasks.
Both agents start at \textit{Front}.
The car's hood needs to be \textit{closed} by the human at last.

\subsection{Qualitative Analysis}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/qualitative_plans.png}
    \caption{
    Plans obtained in two scenarios A and B: Each presents two plans. (\textit{Left}) Obtained by the \textit{existing solver}, and (\textit{Right}) obtained by ours. The latter depicts more realistic and appropriate belief updates (focusing on two attributes).
    Case~(A): the human has no certainty on {\em SaltInPot}, while ours decides to communicate to remove the ambiguity. 
    And, Case~(B): an initial belief divergence on {\em AtPasta} induces an ``invalid'' plan, but ours predicts that the human will \textit{assess} the pasta location, without being communicated.
    }
    \label{fig:scenarios}
\end{figure}

In the first domain, we highlight \textit{subtleties} the \textit{existing solver} overlooks and how \textit{our approach} is aware of them, updates beliefs, and effectively manages divergences. 
We discuss the plans obtained in two scenarios 
as shown in Figure~\ref{fig:scenarios}. Assume that both the agents are in \textit{Kitchen} and the human acts first. Scenario~(A): Captures the agents' plans when the pasta is in \textit{Room}. 
Scenario~(B): 
In hindsight, the pasta is moved from \textit{Room} to {\em Kitchen}, which the robot knows, while the human still has the old belief.

\begin{itemize}
    % \item \textbf{Scenario~(A):} 
    % Although the solvers generate similar plans, they update the human's belief differently if and when the human and robot are co-present. E.g., turning on the stove ideally (realistically) does not affect the human's mental state, which is not the case for the old solver. It considers agents omniscient, so the human knows everything immediately once achieved. 
    % Our solver predicts it when the human returns to the kitchen and assesses that the stove is {\sc on}, and their belief gets updated.
    \item \textbf{Scenario~(A):} 
    % The human leaves the kitchen and hence practically is unaware of the changes achieved in the environment by the robot's actions: {\em turn on} and {\em add salt}. 
    As the human leaves {\em Kitchen}, practically, they are unaware, or at least uncertain, about the changes caused by the robot's actions: {\em turn on} and {\em add salt}. (\textit{Left}) The \textit{existing solver} considers agents omniscient. So, the human knows that the stove is {\sc on}, and salt is added even before arriving back in {\em Kitchen}. (\textit{Right}) Thanks to the \textit{Observation} process, \textit{our approach} estimates that when returning to the kitchen, the human will assess that the stove is {\sc on}. Moreover, it predicts that the human cannot know \textit{SaltInPot} and that this divergence, identified as relevant, needs to be handled via communication.
    % The old solver considers agents omniscient, i.e., they know everything immediately once achieved. Hence, the human knows that the stove is {\sc on}, and salt is added even before being back in \textit{Kitchen}.
    % The new solver, thanks to both an appropriate situation assessment and inference based on action observability, estimates that when returning to the kitchen, the human assesses that the stove is {\sc on}.
    % Moreover, it predicts that the human cannot know the \textit{SaltInPot} fact and that this divergence, identified as relevant, needs to be handled via communication. 
    % This divergence was ineffective after the robot communicates it to the human.
    \item \textbf{Scenario~(B):}
    (\textit{Left}) Based on false belief, the human moves to \textit{Room} to \textit{grab} the pasta, but in reality, being an illegal action w.r.t. the ground truth, it \textit{fails}. 
    % (\textit{Right}) The human agent \textit{assesses} the environment to update their belief, knowing \textit{PastaInKitchen}. Hence, no communication is required.
    (\textit{Right}) 
    % The human \textit{observes} their surrounding and fixes their false belief about \textit{PastaInKitchen}. Hence, no communication is required. 
    The \textit{Observation} process allow us to predict that the human will fix their false belief with perception. Hence, no communication is required. 
    Moreover, the \textit{Inference} process later updates the human belief about \textit{SaltInPot} since the human will be co-present.
\end{itemize} 

% OLD
% \begin{table}
%     \begin{adjustbox}{width=0.77\columnwidth,center}
%     \begin{tabular}{@{}c|r r r| c@{}}
%         % \hline
%         \multirow{2}{*}{
%         \textbf{Domain}} & \multicolumn{3}{c|}{\textbf{\textit{Old Solver}}} & \multicolumn{1}{c}{\textbf{\textit{Our Solver}}}
%         \\
%         % \cline{2-6}
%         & \multicolumn{1}{c}{\textit{S}} & \multicolumn{1}{c}{\textit{NA}} & \multicolumn{1}{c|}{\textit{IDL}} & \multicolumn{1}{c}{\textit{Com}} 
%         \\ \cline{1-5}
%         \textit{Cooking} & 18.6\% & 77.0\% & 23.0\%  & 54.9\%\\
%         \textit{Box} & 25.0\% & 83.3\% & 16.7\%  & 68.8\%\\
%         \textit{Car} & 12.5\% & 73.2\% & 26.8\%  & 79.7\%\\
%         \hline
%         \textbf{Average} & 18.7\% & 77.8\% & 22.2\%  & 67.8\%\\
%     \end{tabular}
%     \end{adjustbox}
%     \caption
%     {
%     \label{tab:q_results}
%     % Comparison of the two solvers on the two domains. 
%     % In each domain: 
%     For the \textit{old solver}, the success rate (\textit{S}), the ratio of failed plans due to a non-applicable action (\textit{NA}), and the ratio of failed plans due to an inactivity deadlock case (\textit{IDL}), while for \textit{our solver}: (the success rate is always 100\%), the ratio of plans including a communication action (\textit{Com}).
%     }
% \end{table}

% V1 %
% \begin{table}
%     \begin{adjustbox}{width=0.77\columnwidth,center}
%     \begin{tabular}{@{}c|c c c| c@{}}
%         \multirow{2}{*}{\textbf{Domain}} & \multicolumn{3}{c|}{\textbf{\textit{Existing Solver}}} & \multicolumn{1}{c}{\textbf{\textit{Our Approach}}}
%         \\
%         & \multicolumn{1}{c}{\textit{S Align. I.B.}} & \multicolumn{1}{c}{\textit{S Div. I.B.}} & \multicolumn{1}{c|}{\textit{S}} & \multicolumn{1}{c}{\textit{Comm}} 
%         \\ \cline{1-5}
%         \textit{Cooking}    & 100\% &  7.0\% & 18.6\% & 54.9\%\\
%         \textit{Box}        & 100\% & 14.3\% & 25.0\% & 68.8\%\\
%         \textit{Car}        & 100\% &  0.0\% & 12.5\% & 83.6\%\\
%         \hline
%         \textbf{Average}    & 100\% &  7.1\% & 18.7\% & 69.1\%\\
%     \end{tabular}
%     \end{adjustbox}
%     \caption
%     {
%     \label{tab:q_results}
%     For the \textit{existing solver}: success rates are shown respectively for problems including aligned initial beliefs (\textit{S Align. I.B.}), diverging initial beliefs (\textit{S Div. I.B.}), and for all problems (\textit{S}). For \textit{our approach}: (the success rate is always 100\%), the ratio of plans including a communication action is shown (\textit{Comm}).
%     }
% \end{table}

% V2
\begin{table}
    \begin{adjustbox}{width=0.77\columnwidth,center}
    \begin{tabular}{@{}c|c c| c@{}}
        \multirow{2}{*}{\textbf{Domain}} & \multicolumn{2}{c|}{\textbf{\textit{Existing Solver}}} & \multicolumn{1}{c}{\textbf{\textit{Our Approach}}}
        \\
        & \multicolumn{1}{c}{\textit{S I.Div.B.}} & \multicolumn{1}{c|}{\textit{S}} & \multicolumn{1}{c}{\textit{Comm}} 
        \\ \cline{1-4}
        \textit{Cooking}    &  7.0\% & 18.6\% & 54.9\%\\
        \textit{Box}        & 14.3\% & 25.0\% & 68.8\%\\
        \textit{Car}        &  0.0\% & 12.5\% & 83.6\%\\
        \hline
        \textbf{Average}    &  7.1\% & 18.7\% & 69.1\%\\
    \end{tabular}
    \end{adjustbox}
    \caption
    {
    \label{tab:q_results}
    For the \textit{existing solver}: the success rate for problems including initially aligned beliefs is always 100\%, the success rate for problems with initially diverging beliefs (\textit{S I.Div.B.}), and overall success rate (\textit{S}). For \textit{our approach}: the success rate is always 100\%, and the ratio of plans including a communication action (\textit{Comm}). 
    % The table depicts how poorly the existing solver handles belief divergences in constrast to our approach that doesn't communicate systematically.
    The existing solver poorly handles belief divergences in contrast to our approach which doesn't communicate systematically.
    }
\end{table}

\subsection{Experimental Results and Analysis}

% A problem is defined by a starting agent (Human or Robot) and by a pair of initial Human-Robot beliefs.
In each domain, the actions and tasks remain the same. So here, a problem is defined by a starting agent ($\varphi_r$ or $\varphi_h$) and a pair of initial beliefs ($B_{\varphi_r}^{s_0}, B_{\varphi_h}^{s_0}$).
Initial ground truth ($B_{\varphi_r}^{s_0}$) is defined by setting each attribute to an initial value. But, 5 selected attributes can be set to 2 possible values instead of 1. And among these selected attributes, 3 can diverge in the human belief, generating 256 pairs of initial beliefs. Hence, 12.5\% of the generated problems include initially aligned beliefs. Then, by taking into account the starting agent, we obtain 512 problems for each domain. 
Each of the 1536 generated problems has been solved by the \textit{existing solver} and \textit{our approach} generating a total of 3072 plans. The obtained quantitative results appear in Table~\ref{tab:q_results}.
% Note that in the Box domain, we considered \textit{three} boxes to be prepared and sent. 
Three boxes has been considered in the Box domain.

% With the old approach, a planning failure occurs due to: 
% (a) an action of a plan not applicable in another agent's belief state, including the ground truth; 
% (b) if an inactivity deadlock occurs, which is assumed to be the case after a succession of at least four {\em WAIT} and (or) {\em IDLE} actions. 
% Such deadlocks occur when the human has a belief divergence and waits for a never-happening robot's action, e.g., waiting for \textit{adding} salt, but \textit{SaltInPot} is already achieved.
% For the old solver, the success rate (\textit{S}), the \textit{ratios} of the number of failed plans due to a inapplicable action (\textit{NA}) and to an inactivity deadlock (\textit{IDL}) appear in the table.
% For ours, the \textit{ratio} of successful plans with a communication action is presented under \textit{Com}.

% As the old solver does not handle belief divergence in planning, the applicability of actions is never an issue w.r.t. another agent's beliefs. 
% Therefore, if the \textit{IDL} case occurs, it is understood that the initial belief divergences are not tackled by modeling triggers explicitly in the task specifications. 

% Our solver always finds legal plans, and on average $\approx$ 69\% of them use \textit{communication}.
% Moreover, the robot doesn't need to communicate systematically as assessing situations handles a major part of the divergences (87.5\% of the scenarios have divergent beliefs initially).
% For the old solver, if no initial belief divergence exists, it always finds a legal plan, considering the agents omniscient. 
% E.g., Fig.~\ref{fig:scenarios}(A). However, sometimes, this causes problems in practice.
% Scenarios beginning with unaligned beliefs induce actions often not applicable in another agent's belief state (or the ground truth), evident by the (average) 18.7\% success rate.

% We can see that our solver uses more communication actions in the car maintenance domain than in the other two.
% But looking at the \textit{low} old solver's success rate (12.5\%), we conclude that this domain generally needs more communication. 
% Compared to others, it purposely creates non-relevant belief divergence, e.g., the robot storing the gallon in the cabinet ($\inferable$) is irrelevant.
% After a close analysis, we found that $\approx$ 42\% of the plans have divergent human beliefs at least in one branch after the last action (\textit{leaf}) was executed (i.e., \textit{irrelevant} divergences never got communicated). 

The \textit{existing solver} always finds legal plans when dealing with aligned beliefs. But as expected, the low success rate for initially diverging beliefs (7.1\%) reflects how the \textit{existing solver} poorly handles belief divergences without specifically designed action models. Since \textit{our approach} always finds legal plans, we can say it solves a broader class of problems.

% Furthermore, 87.5\% of all problems initially include belief divergences. And even when starting with aligned beliefs, some divergences can be created along the planning process. Thus, we can state that more than 87.5\% of all problems involve belief divergences. 
% And even when starting with aligned beliefs, some divergences can be created along the planning process. Thus, we can state that more than 87.5\% of all problems involve belief divergences.
% However, only 69.1\% of the generated plans include communication actions. This means that \textit{our approach} communicates only when necessary, and not systematically.   
Furthermore, considering the initially diverging beliefs and the divergences created along the planning process, more than 87.5\% of all problems involve belief divergences. 
However, only 69.1\% of the generated plans include communication actions. This means that \textit{our approach} communicates only when necessary, and not systematically.   

\section{Discussion} 
% \& Conclusion} 
\label{sec:disc-conc}
% We formalize execution-time observability conventions based on situation assessment and action observability (ToM). We use it to estimate the evolution of the human mental state and capture belief divergences. 
% A new planner is described, which utilizes this better estimation of human mental state to plan for more robust and consistent human-robot joint activities such that a relevant belief divergence is tackled by explicitly modeled communication actions.  

% Moreover, 
% We put an effort to make our \textit{contribution} to have a ``generic'' description. Be believe they are not limited to only the underlying framework.
% We believe our observability and SA models are not limited to the underlying framework. So we put an effort to make our \textit{contribution} to have a ``generic" description.

We believe our modeling-level SA proposals could fit in any other planning approach framing multi-party systems having one controllable agent while can only hypothesize remaining agents' behaviors  (e.g., human-centered AI). Thus, we put an effort to give our \textit{contribution} a ``generic" description.

Handling SA formally is vital for robust planning, as otherwise, it can lead to problems at run-time. 
E.g., Fig.~\ref{fig:scenarios}(A): the human knowing \textit{SaltInPot} is ambiguous. 
Explicit reasoning on the human mental state detects ambiguous situations and removes them via communication. 

Our formalism cannot refute something believed by an agent through SA, it can only assess new truth. 
% E.g., for some $s_i \in \mathcal{S}$, 
For instance, 
assume the human \textit{wrongly} believes that the pasta is in \textit{Kitchen}. The SA does not help refute this, even if the human is in \textit{Kitchen}. 
The reason is that $f_{\textit{PastaNotInKitchen}}^{s_i}(...)$ is not modeled explicitly as an attribute. 
And hence such issues do not affect the completeness. 
Moreover, if necessary, \textit{our approach} handles such cases as \textit{relevant} divergences and communicates to the human.

% \pagebreak
\bibliography{bib.bib}

\end{document}


